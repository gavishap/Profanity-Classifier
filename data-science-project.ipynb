{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aab059c",
   "metadata": {
    "papermill": {
     "duration": 0.153619,
     "end_time": "2021-10-20T14:51:54.842196",
     "exception": false,
     "start_time": "2021-10-20T14:51:54.688577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c03343",
   "metadata": {
    "papermill": {
     "duration": 0.150693,
     "end_time": "2021-10-20T14:51:55.147461",
     "exception": false,
     "start_time": "2021-10-20T14:51:54.996768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This project is made by Open University Computer Science students Vitaly Chait and Gabriel Shapiro. We debated on what our project should be about and we landed on the idea of profanity identification within text. We both saw the need for this type of technology to be implemented within the social media universe and we were both very interested to get to work.\n",
    "\n",
    "![Profanity_intro](https://www.theoakleafnews.com/wp-content/uploads/2018/04/Profanity-illustration-900x675.jpg)\n",
    "\n",
    "## The Problem / Solution\n",
    "\n",
    "There is a constant rise to the number of devices connected to the web (IOT) and the content being spread by different people across the globe. Also, the starting age of the user is constantly decreasing to our new reality, a reality where every elementary school and even kindergarten kids are already surfing in the open web alone without any parent supervising their activity. \n",
    "\n",
    "### This leads us to the idea of our final data science & machine learning project.\n",
    "The project will create a scoring system, that will give a pass \\ no pass to content loading to its interface. Underage browsers will be able to see content that is suitable for their age by only including text that matches their threshold of profanity. There are many obstacles that must be tested for in order for the model to have a good reliability rating of correct classification. The english language is very safisticated with its grammar, and the meaning of a sentence can change with one word or one symbol. We will be working with datasets of collected sentences from the internet that we will be able to register inside our testing model and classify each sentence with a profanity grade.\n",
    "\n",
    "![Profanity_intro](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305282cc",
   "metadata": {
    "papermill": {
     "duration": 0.151768,
     "end_time": "2021-10-20T14:51:55.449972",
     "exception": false,
     "start_time": "2021-10-20T14:51:55.298204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. INIT and prerequisites "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f20190",
   "metadata": {
    "papermill": {
     "duration": 0.150035,
     "end_time": "2021-10-20T14:51:55.750506",
     "exception": false,
     "start_time": "2021-10-20T14:51:55.600471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[Download and install Visual Studio](https://visualstudio.microsoft.com/downloads/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41707396",
   "metadata": {
    "papermill": {
     "duration": 0.147876,
     "end_time": "2021-10-20T14:51:56.047967",
     "exception": false,
     "start_time": "2021-10-20T14:51:55.900091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### If you have an Nvidia GPU you are welcomed to download CUDA \n",
    "[CUDA](https://developer.nvidia.com/cuda-downloads) + [cudnn](https://developer.nvidia.com/cudnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d74dad",
   "metadata": {
    "papermill": {
     "duration": 0.150884,
     "end_time": "2021-10-20T14:51:56.398630",
     "exception": false,
     "start_time": "2021-10-20T14:51:56.247746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### @Quick Start Parameters - set True or False the parameter below for activation\n",
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083276c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:51:56.776541Z",
     "iopub.status.busy": "2021-10-20T14:51:56.774532Z",
     "iopub.status.idle": "2021-10-20T14:51:56.786144Z",
     "shell.execute_reply": "2021-10-20T14:51:56.786733Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.907741Z"
    },
    "papermill": {
     "duration": 0.16926,
     "end_time": "2021-10-20T14:51:56.787047",
     "exception": false,
     "start_time": "2021-10-20T14:51:56.617787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kaggle = True\n",
    "SHORT_RUN = False\n",
    "LONG_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f94581a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:51:57.153638Z",
     "iopub.status.busy": "2021-10-20T14:51:57.152524Z",
     "iopub.status.idle": "2021-10-20T14:51:57.154788Z",
     "shell.execute_reply": "2021-10-20T14:51:57.155365Z",
     "shell.execute_reply.started": "2021-10-20T14:49:56.399469Z"
    },
    "papermill": {
     "duration": 0.16121,
     "end_time": "2021-10-20T14:51:57.155517",
     "exception": false,
     "start_time": "2021-10-20T14:51:56.994307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_path = \"\"\n",
    "scrapy_path= \"\"\n",
    "if kaggle:\n",
    "    input_path = r\"../input/\" + input_path\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.abspath(r\"../input/scrapygoodnews/scrapygoodnews/\"))\n",
    "    sys.path.append(os.path.abspath(r\"../input/scrapygoodnews/scrapygoodnews/spiders/\"))\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5a3db",
   "metadata": {
    "papermill": {
     "duration": 0.152446,
     "end_time": "2021-10-20T14:51:57.458949",
     "exception": false,
     "start_time": "2021-10-20T14:51:57.306503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61895c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:51:57.803565Z",
     "iopub.status.busy": "2021-10-20T14:51:57.783083Z",
     "iopub.status.idle": "2021-10-20T14:52:20.505814Z",
     "shell.execute_reply": "2021-10-20T14:52:20.502859Z",
     "shell.execute_reply.started": "2021-10-20T14:48:45.642417Z"
    },
    "papermill": {
     "duration": 22.893974,
     "end_time": "2021-10-20T14:52:20.506214",
     "exception": true,
     "start_time": "2021-10-20T14:51:57.612240",
     "status": "failed"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "\n",
      "Python version:                         3.7.10\n",
      "Collecting Scrapy\r\n",
      "  Downloading Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 254 kB 545 kB/s \r\n",
      "\u001b[?25hCollecting h2<4.0,>=3.0\r\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 65 kB 1.6 MB/s \r\n",
      "\u001b[?25hCollecting queuelib>=1.4.2\r\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\r\n",
      "Collecting PyDispatcher>=2.0.5\r\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: zope.interface>=4.1.3 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (5.4.0)\r\n",
      "Requirement already satisfied: Twisted[http2]>=17.9.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (21.7.0)\r\n",
      "Collecting w3lib>=1.17.0\r\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\r\n",
      "Collecting itemloaders>=1.0.1\r\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\r\n",
      "Collecting itemadapter>=0.1.0\r\n",
      "  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\r\n",
      "Collecting service-identity>=16.0.0\r\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting protego>=0.1.15\r\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 2.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (3.4.7)\r\n",
      "Requirement already satisfied: lxml>=3.5.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (4.6.3)\r\n",
      "Collecting cssselect>=0.9.1\r\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\r\n",
      "Collecting parsel>=1.5.0\r\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\r\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (20.0.1)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.0->Scrapy) (1.14.6)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.20)\r\n",
      "Collecting hpack<4,>=3.0\r\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\r\n",
      "Collecting hyperframe<6,>=5.2.0\r\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.7/site-packages (from itemloaders>=1.0.1->Scrapy) (0.10.0)\r\n",
      "Requirement already satisfied: six>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from parsel>=1.5.0->Scrapy) (1.16.0)\r\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->Scrapy) (0.2.7)\r\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->Scrapy) (21.2.0)\r\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.7/site-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\r\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->Scrapy) (21.0.0)\r\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->Scrapy) (15.1.0)\r\n",
      "Requirement already satisfied: incremental>=21.3.0 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->Scrapy) (21.3.0)\r\n",
      "Requirement already satisfied: Automat>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->Scrapy) (20.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from Twisted[http2]>=17.9.0->Scrapy) (3.10.0.2)\r\n",
      "Collecting priority<2.0,>=1.1.0\r\n",
      "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Requirement already satisfied: idna>=2.5 in /opt/conda/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->Scrapy) (2.10)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from zope.interface>=4.1.3->Scrapy) (58.0.4)\r\n",
      "Building wheels for collected packages: protego, PyDispatcher\r\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7782 sha256=4aac8418574dee2e19b9491feef43c946f6cf8dbe82de33a82ba39165124596a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\r\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=6a3a147c0314cb961608046c5744b1727e6c9a119d028a168334c826f89409a8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\r\n",
      "Successfully built protego PyDispatcher\r\n",
      "Installing collected packages: w3lib, hyperframe, hpack, cssselect, priority, parsel, itemadapter, h2, service-identity, queuelib, PyDispatcher, protego, itemloaders, Scrapy\r\n",
      "Successfully installed PyDispatcher-2.0.5 Scrapy-2.5.1 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 itemadapter-0.4.0 itemloaders-1.0.4 parsel-1.6.0 priority-1.3.0 protego-0.1.16 queuelib-1.6.2 service-identity-21.1.0 w3lib-1.22.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scrapy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17/2992244051.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install Scrapy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scrapy version: {:>30}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scrapy' is not defined"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "try:\n",
    "    os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin\")\n",
    "except:\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Python version: {:>30}\".format(platform.python_version()))\n",
    "\n",
    "try:\n",
    "    import scrapy\n",
    "except:\n",
    "    !pip3 install Scrapy\n",
    "print(\"scrapy version: {:>30}\".format(scrapy.__version__))\n",
    "    \n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    !pip3 install cv2\n",
    "print(\"OpenCV version: {:>30}\".format(cv2.__version__))\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "except:\n",
    "    !pip3 install pandas\n",
    "print(\"pandas version: {:>30}\".format(pandas.__version__))\n",
    "\n",
    "try:\n",
    "    import PIL\n",
    "except:\n",
    "    !pip3 install pillow\n",
    "print(\"PIL version: {:>33}\".format(PIL.__version__))\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except:\n",
    "    !pip3 install numpy\n",
    "print(\"numpy version: {:>32}\".format(numpy.__version__))\n",
    "    \n",
    "try:\n",
    "    import joblib\n",
    "except:\n",
    "    !pip3 install joblib\n",
    "print(\"joblib version: {:>30}\".format(joblib.__version__))\n",
    "    \n",
    "try:\n",
    "    import sklearn\n",
    "except:\n",
    "    !pip3 install sklearn    \n",
    "print(\"sklearn version: {:>30}\".format(sklearn.__version__))\n",
    "\n",
    "try:\n",
    "    import tensorflow\n",
    "except:\n",
    "    !pip3 install tensorflow \n",
    "print(\"tensorflow version: {:>26}\".format(tensorflow.__version__))\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "except:\n",
    "    !pip3 install matplotlib\n",
    "print(\"matplotlib version: {:>26}\".format(matplotlib.__version__))   \n",
    "\n",
    "try:\n",
    "    import IPython\n",
    "except:\n",
    "    !pip3 install IPython\n",
    "print(\"IPython version: {:>30}\".format(IPython.__version__))\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip3 install spacy\n",
    "print(\"spacy version: {:>31}\".format(spacy.__version__))\n",
    "\n",
    "try:\n",
    "    import seaborn\n",
    "except:\n",
    "    !pip3 install seaborn\n",
    "print(\"seaborn version: {:>30}\".format(seaborn.__version__))\n",
    "\n",
    "try:\n",
    "    import statsmodels\n",
    "except:\n",
    "    !pip3 install statsmodels\n",
    "print(\"statsmodels version: {:>26}\".format(statsmodels.__version__))  \n",
    "\n",
    "try:\n",
    "    import gensim\n",
    "except:\n",
    "    !pip3 install gensim\n",
    "print(\"gensim version: {:>30}\".format(gensim.__version__))  \n",
    "\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip3 install statsmodels\n",
    "print(\"nltk version: {:>32}\".format(nltk.__version__)) \n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except:\n",
    "    !pip3 install requests\n",
    "print(\"requests version: {:>29}\".format(requests.__version__)) \n",
    "\n",
    "try:\n",
    "    import statsmodels\n",
    "except:\n",
    "    !pip3 install statsmodels\n",
    "print(\"statsmodels version: {:>26}\".format(statsmodels.__version__))\n",
    "\n",
    "try:\n",
    "    import tqdm\n",
    "except:\n",
    "    !pip3 install tqdm\n",
    "print(\"tqdm version: {:>33}\".format(tqdm.__version__))\n",
    "\n",
    "try:\n",
    "    import pytesseract\n",
    "except:\n",
    "    !pip3 install pytesseract\n",
    "print(\"pytesseract version: {:>25}\".format(pytesseract.__version__))\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "except:\n",
    "    !pip3 install transformers\n",
    "print(\"transformers version: {:>24}\".format(transformers.__version__))\n",
    "\n",
    "try:\n",
    "    import bertviz\n",
    "except:\n",
    "    !pip3 install bertviz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display\n",
    "import multiprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab81ed7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"data:image/jpeg;base64,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\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b01a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### @GPU_ENABLED - set True or False the parameter below for activation\n",
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbf5ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:48:58.472296Z",
     "iopub.status.busy": "2021-10-20T14:48:58.472027Z",
     "iopub.status.idle": "2021-10-20T14:48:58.476278Z",
     "shell.execute_reply": "2021-10-20T14:48:58.475454Z",
     "shell.execute_reply.started": "2021-10-20T14:48:58.472268Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GPU_ENABLED = True # Change to False if you don't have GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7673a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:49:26.226249Z",
     "iopub.status.busy": "2021-10-20T14:49:26.225359Z",
     "iopub.status.idle": "2021-10-20T14:49:26.240668Z",
     "shell.execute_reply": "2021-10-20T14:49:26.239879Z",
     "shell.execute_reply.started": "2021-10-20T14:49:26.226201Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if GPU_ENABLED:\n",
    "    physical_devices = tensorflow.config.list_physical_devices('GPU')\n",
    "    print(\"Num GPUs:\", len(physical_devices))\n",
    "    try:\n",
    "        spacy.prefer_gpu()\n",
    "    except:\n",
    "        print(\"Not able to activate gpu\")\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(\"Num CPUs:\", cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02711163",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Hi reader, if you wish to optimize performence more you can read the link below\n",
    "[Feel free to read more about optimizations](https://spacy.io/usage/processing-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fb4b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://hlassets.paessler.com/common/files/preview/sql-monitoring.png\" alt=\"Drawing\" style=\"width: 12000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138569fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 2. Data and Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577102cd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Scrapy crawling data collection\n",
    "<img src=\"data:image/png;base64,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\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "##### @Crawl - Set this parameter to \"True\" or \"False\" if you wish to activate it\n",
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb1547",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:54.988531Z",
     "iopub.status.idle": "2021-10-20T14:34:54.988837Z",
     "shell.execute_reply": "2021-10-20T14:34:54.988702Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.988682Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "activate_crawl = False\n",
    "if SHORT_RUN == True:\n",
    "    activate_crawl = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173550b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Scrappy code below. \n",
    "Blocks format is (\"file_name.py\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41879a73",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### 2.1.1.1  \"items.py\" file \n",
    "--> scrapygoodnews\\scrapygoodnews\\items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc285c1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:54.989784Z",
     "iopub.status.idle": "2021-10-20T14:34:54.990666Z",
     "shell.execute_reply": "2021-10-20T14:34:54.990496Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.990476Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ScrapygoodnewsItem(scrapy.Item):\n",
    "    story = scrapy.Field()\n",
    "    url = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd2c55",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### 2.1.1.2  \"goodnews_scrape.py\" file \n",
    "--> scrapygoodnews\\scrapygoodnews\\spiders\\goodnews_scrape.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5e5f8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:54.991817Z",
     "iopub.status.idle": "2021-10-20T14:34:54.992596Z",
     "shell.execute_reply": "2021-10-20T14:34:54.992422Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.992399Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class Goodnews(scrapy.Spider):\n",
    "    name = \"my_scraper\"\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'scrapygoodnews\\scrapygoodnews\\output\\stories.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }}\n",
    "\n",
    "    allowed_domains = ['www.goodnewsnetwork.org']\n",
    "    # First Start Url\n",
    "    start_urls = [\"https://www.goodnewsnetwork.org/category/news/page/1/\"]\n",
    "    n_pages = 10**5\n",
    "\n",
    "    for i in range(2, n_pages):\n",
    "        start_urls.append(\"https://www.goodnewsnetwork.org/category/news/page/\" + str(i))\n",
    "\n",
    "    def parse(self, response):\n",
    "        for href in response.xpath(\n",
    "                '//h3[@class=\"entry-title td-module-title\"]//@href').extract():\n",
    "            yield scrapy.Request(href, callback=self.parse_dir_contents)\n",
    "\n",
    "    def parse_dir_contents(self, response):\n",
    "        item = ScrapygoodnewsItem()\n",
    "\n",
    "        # Getting Story\n",
    "        story_list = response.xpath('//div[@class=\"td-post-content\"]//p/text()').extract()\n",
    "        story_list = [x.strip() for x in story_list if len(x.strip()) > 0]\n",
    "\n",
    "        if len(story_list) > 0:\n",
    "            item['story'] = \" \".join(story_list)# Url (The link to the page)\n",
    "            item['url'] = response.xpath(\"//meta[@property='og:url']/@content\").extract()\n",
    "            yield item\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf2830",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### 2.1.1.3 settings.py --> \\scrapygoodnews\\scrapygoodnews\\settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79971d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:54.994028Z",
     "iopub.status.idle": "2021-10-20T14:34:54.994313Z",
     "shell.execute_reply": "2021-10-20T14:34:54.994183Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.994169Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrapy settings for scrapygoodnews project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "import logging\n",
    "\n",
    "BOT_NAME = 'scrapygoodnews'\n",
    "\n",
    "SPIDER_MODULES = ['spiders']\n",
    "NEWSPIDER_MODULE = 'spiders'\n",
    "    \n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "LOG_LEVEL = logging.WARNING\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'scrapygoodnews.middlewares.ScrapygoodnewsSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'scrapygoodnews.middlewares.ScrapygoodnewsDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'scrapygoodnews.pipelines.ScrapygoodnewsPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1715d94",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### 2.1.1.4 run_spider_file.py \n",
    "--> \\scrapygoodnews\\scrapygoodnews\\run_spider_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bee60d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:54.995771Z",
     "iopub.status.idle": "2021-10-20T14:34:54.996087Z",
     "shell.execute_reply": "2021-10-20T14:34:54.995949Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.995930Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if kaggle: \n",
    "    import settings as my_settings\n",
    "    from scrapy.settings import Settings\n",
    "    from scrapy.crawler import CrawlerProcess\n",
    "    \n",
    "    \n",
    "    \n",
    "stop_after_crawl = True\n",
    "\n",
    "def run_spider():\n",
    "    \"\"\"run spider with Goodnews\"\"\"\n",
    "    # Import settings from project and not terminal default path\n",
    "    crawler_settings = Settings()\n",
    "    crawler_settings.setmodule(my_settings)\n",
    "    crawler_settings.update({\"SPIDER_MODULES\": SPIDER_MODULES})\n",
    "    crawler_settings.update({\"NEWSPIDER_MODULE\": NEWSPIDER_MODULE})\n",
    "\n",
    "    crawler = CrawlerProcess(crawler_settings)\n",
    "    # Avoid Twisted reactor issue - For running the same notebook\n",
    "    print(\"Spider start running\\n /╲/\\\\(╭ •̀ •́╮)/\\\\╱\\\\ \\t /╲/\\\\(╭ •̀ •́╮)/\\╱\\\\ \\t /╲/\\\\(╭ •̀ •́╮)/\\\\╱\\\\\")\n",
    "    crawler.crawl(Goodnews)\n",
    "    crawler.start(stop_after_crawl=stop_after_crawl)\n",
    "    print(\"Spider end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed466fec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:54.997190Z",
     "iopub.status.idle": "2021-10-20T14:34:54.997502Z",
     "shell.execute_reply": "2021-10-20T14:34:54.997342Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.997327Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_crawl:\n",
    "    run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df793ecb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://www.springboard.com/library/static/fc5022f0026f5380ffc5802111b21527/bbc2a/DSC-Article-DSC-Roles-DataScientist.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acdde0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2.2 The Data\n",
    "\n",
    "We will begin with explaining the datasets that we are working with. These are built quite differently.\n",
    "\n",
    "\n",
    "#### 2.2.1 Data_a\n",
    "The dataset contains twitter comments with a class column that that gives 1 if there is offensive language, 0 if there is hate speech and 2 if there is neither.\n",
    "\n",
    "\n",
    "#### 2.2.2 Data_b\n",
    "The dataset is from wikipedia texts classifies whether each text is toxic speech or threatining speech or other types, and we were able to take that and say that if any of those classifications exist that we can label it as profane language.\n",
    "\n",
    "\n",
    "#### 2.2.3 Data_c\n",
    "We will also use data from sources that were not manually labeled as part of a sponsored project (Keggle/etc..), this type of data is generated from known sources with high rate of success being correct without manual verification. \n",
    "We used scrappy opensource package to crawl \"https://www.goodnewsnetwork.org/\" and extract the text from the articles that were posted there. \n",
    "\n",
    "\n",
    "#### 2.2.4 Data_d, e, f, g, h\n",
    "Random lists of bad words we found online that come in different formats\n",
    "\n",
    "####  2.2.5 Data - Source URLS\n",
    "\n",
    "##### 2.2.5.1 Labled Datasets\n",
    "[Database 1 - Source](https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data)\n",
    "\n",
    "[Database 2 - Source](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/)\n",
    "\n",
    "\n",
    "##### 2.2.5.2 Scrapy\n",
    "[The good news network website](https://www.goodnewsnetwork.org/more/about-us/)\n",
    "\n",
    "\n",
    "##### 2.2.5.3 Bad words lists\n",
    "[DB4](https://github.com/web-mech/badwords/blob/master/lib/lang.json)\n",
    "[DB5](http://www.bannedwordlist.com/)\n",
    "[DB6](https://www.freewebheaders.com/bad-words-list-and-page-moderation-words-list-for-facebook/)\n",
    "[DB7](https://www.freewebheaders.com/youtube-blacklist-words-list-youtube-comment-moderation/)\n",
    "[DB8](https://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/)\n",
    "\n",
    "\n",
    "<img src=\"https://ugc.futurelearn.com/uploads/images/b2/8f/b28f7f9d-1343-4bb7-9c09-9c3beb76fad7.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08288f0b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Handling the data\n",
    "\n",
    "After we gather the datasets that we want to use we have to clean the text of any superfluous characters that will not help us with detremining the sentiment of the sentence. \n",
    "The template should be some how similiar to \"yes \\ no\" of whether the text (sentences) are offensive or not.\n",
    "\n",
    "So the goal is that for each row  we shall provide a binary indexing of \"Offensive\" or not. \n",
    "To have a uniform text template we shall use filtering techniques such as splits of the parahraph to sentences, tokenizations, characters removals and more. \n",
    "In addition we shall add another column of the words counts. This can help with determing the \"weight\" of the word on the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba6962",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fffa25",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:54.999162Z",
     "iopub.status.idle": "2021-10-20T14:34:55.000269Z",
     "shell.execute_reply": "2021-10-20T14:34:54.999991Z",
     "shell.execute_reply.started": "2021-10-20T14:34:54.999958Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "activate_db_filtering = True\n",
    "if SHORT_RUN == True:\n",
    "    activate_db_filtering = False\n",
    "if LONG_RUN == True:\n",
    "    activate_db_filtering = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355db2fb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2.3.1 Importing the required packages and loading all the data from the disk into Pandas framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a57c07",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.001638Z",
     "iopub.status.idle": "2021-10-20T14:34:55.002130Z",
     "shell.execute_reply": "2021-10-20T14:34:55.001878Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.001849Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy import load \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4620a41",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.006711Z",
     "iopub.status.idle": "2021-10-20T14:34:55.007202Z",
     "shell.execute_reply": "2021-10-20T14:34:55.006973Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.006947Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_a = pd.read_csv(input_path+'materials//Cornnel//data//data//labeled_data.csv')\n",
    "    data_b = pd.read_csv(input_path+'materials//Kaggle//train//train.csv')\n",
    "    data_c = pd.read_csv(input_path+'scrapygoodnews//scrapygoodnews//output//stories.csv')\n",
    "    data_d = pd.read_json(input_path+'materials//badwords//word_list_a.json')\n",
    "    data_e = pd.read_csv(input_path+'materials//badwords//swearWords.csv')\n",
    "    data_f = pd.read_csv(input_path+'materials//badwords//format_b//facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt', sep=\"\\n\")\n",
    "    data_g = pd.read_csv(input_path+'materials//badwords//format_b//youtube-blacklist-words-list_comma-separated-text-file_2021-01-19.txt', sep=\"\\n\")\n",
    "    data_h = pd.read_csv(input_path+'materials//badwords//full-list-of-bad-words_csv-file_2021_01_18//full-list-of-bad-words_csv-file_2021_01_18.csv')\n",
    "else:\n",
    "    balanced = pd.read_csv(input_path+'balanced//balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f9fdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The format of is:\n",
    "1. Have a peak on the data\n",
    "2. Fine tune it\n",
    "3. Have another peak on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e825d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://www.star-spain.com/sites/default/files/Data%20record%20filtering%20for%20terminology%20management.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a3a8e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 2.3.1.1 Data - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ad9be",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.008658Z",
     "iopub.status.idle": "2021-10-20T14:34:55.009204Z",
     "shell.execute_reply": "2021-10-20T14:34:55.008927Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.008876Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_a.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5da79",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "By peaking on the database structure, we see that it has more than 2 classifications classes. The first step is to transform it to binary (Offensive or not offensive?).\n",
    "Later, we see that this data was imported from a database with Tweets, it makes sense to use the NLTK TweetTokenizer to handel the data.\n",
    "Finally we use some regular expression replacments to remove unwanted string sequences from the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b492f05",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.010845Z",
     "iopub.status.idle": "2021-10-20T14:34:55.011328Z",
     "shell.execute_reply": "2021-10-20T14:34:55.011108Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.011082Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_a_positive = data_a[data_a[\"class\"]==2]\n",
    "    data_a_positive = pd.DataFrame(data_a_positive[\"tweet\"])\n",
    "    data_a_positive = data_a_positive.rename(columns={\"tweet\": \"Text\"})\n",
    "    data_a_positive = data_a_positive.assign(Negative=[0 for i in range(len(data_a_positive))])\n",
    "\n",
    "    data_a_negative = data_a[data_a[\"class\"]!=2]\n",
    "    data_a_negative = pd.DataFrame(data_a_negative[\"tweet\"])\n",
    "    data_a_negative = data_a_negative.rename(columns={\"tweet\": \"Text\"})\n",
    "    data_a_negative = data_a_negative.assign(Negative=[1 for i in range(len(data_a_negative))])\n",
    "\n",
    "\n",
    "    data_a_labeled = pd.concat([data_a_positive, data_a_negative], axis=0)\n",
    "\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "    data_a_labeled['Text'] = data_a_labeled['Text'].apply(tknzr.tokenize)\n",
    "    data_a_labeled['Text'] = data_a_labeled['Text'].apply(\" \".join)\n",
    "\n",
    "    data_a_labeled[\"Text\"] = data_a_labeled[\"Text\"].str.replace('\\n', ' ', regex=False)\n",
    "    data_a_labeled[\"Text\"] = data_a_labeled[\"Text\"].str.replace(r'(\"|! ! ! rt :)' , '', regex=True)\n",
    "    data_a_labeled[\"Text\"] = data_a_labeled[\"Text\"].str.replace(r'(rt : )' , '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd84464",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.012765Z",
     "iopub.status.idle": "2021-10-20T14:34:55.013271Z",
     "shell.execute_reply": "2021-10-20T14:34:55.013049Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.013026Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_a_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7397c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 2.3.1.2 Data - B \n",
    "The data is slighly different but the general idea is the same. Transform to binary classes and remove unwated string sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814bc71",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.014961Z",
     "iopub.status.idle": "2021-10-20T14:34:55.015414Z",
     "shell.execute_reply": "2021-10-20T14:34:55.015203Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.015180Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_b.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e15010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:34:55.022581Z",
     "iopub.status.busy": "2021-10-20T14:34:55.022233Z",
     "iopub.status.idle": "2021-10-20T14:34:55.048144Z",
     "shell.execute_reply": "2021-10-20T14:34:55.046822Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.022537Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_b_positive = data_b.loc[(data_b['toxic']==0) & (data_b['severe_toxic']==0) & (data_b['obscene']==0) & (data_b['threat']==0) \n",
    "                                & (data_b['insult']==0) & (data_b['identity_hate']==0)]\n",
    "\n",
    "    data_b_positive = pd.DataFrame(data_b_positive[\"comment_text\"])\n",
    "    data_b_positive = data_b_positive.rename(columns={\"comment_text\": \"Text\"})\n",
    "    data_b_positive = data_b_positive.assign(Negative=[0 for i in range(len(data_b_positive))])\n",
    "\n",
    "    data_b_negative = data_b.loc[(data_b['toxic']==1) | (data_b['severe_toxic']==1) | (data_b['obscene']==1) | (data_b['threat']==1) \n",
    "                                | (data_b['insult']==1) | (data_b['identity_hate']==1)]\n",
    "    data_b_negative = pd.DataFrame(data_b_negative[\"comment_text\"])\n",
    "    data_b_negative = data_b_negative.rename(columns={\"comment_text\": \"Text\"})\n",
    "    data_b_negative = data_b_negative.assign(Negative=[1 for i in range(len(data_b_negative))])\n",
    "\n",
    "\n",
    "    data_b_labeled = pd.concat([data_b_positive, data_b_negative], axis=0)\n",
    "    data_b_labeled[\"Text\"] = data_b_labeled[\"Text\"].str.replace('\\n', ' ', regex=False)\n",
    "    data_b_labeled[\"Text\"] = data_b_labeled[\"Text\"].str.replace('\"', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257f98f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.049167Z",
     "iopub.status.idle": "2021-10-20T14:34:55.049657Z",
     "shell.execute_reply": "2021-10-20T14:34:55.049427Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.049401Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_b_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061195e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 2.3.1.3 Data - C\n",
    "Our Scrapy collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0ffd6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.052335Z",
     "iopub.status.idle": "2021-10-20T14:34:55.052954Z",
     "shell.execute_reply": "2021-10-20T14:34:55.052610Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.052583Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_c.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48dd450",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### 2.3.1.3.1 Porter Stemming \n",
    "We can use porter stemming to reduce the complexity, I eventually chose not to use it\n",
    "\n",
    "<img src=\"http://ksabnajdorperp.janbasktraining.in/community/uploads/4af37b87638ff2b7386c7e5764d2fa12.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e04040",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4266a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.054716Z",
     "iopub.status.idle": "2021-10-20T14:34:55.055237Z",
     "shell.execute_reply": "2021-10-20T14:34:55.054997Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.054969Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "porter_filtering = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d33278",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.056972Z",
     "iopub.status.idle": "2021-10-20T14:34:55.057476Z",
     "shell.execute_reply": "2021-10-20T14:34:55.057228Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.057202Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_c_story = pd.DataFrame(data_c[\"story\"])\n",
    "    splitted_data = [] \n",
    "    \n",
    "    if porter_filtering:\n",
    "        porter = PorterStemmer()\n",
    "        for text in data_c_story[\"story\"]:\n",
    "            splitted_sent = sent_tokenize(text)\n",
    "            for sent in splitted_sent:\n",
    "                token_words = word_tokenize(sent)\n",
    "                portered = [porter.stem(word) for word in token_words]\n",
    "                splitted_data.append(\" \".join(portered))\n",
    "    else:\n",
    "        for text in data_c_story[\"story\"]:\n",
    "            splitted = sent_tokenize(text)\n",
    "            for i in splitted:\n",
    "                splitted_data.append(i)\n",
    "    \n",
    "    splitted_data = pd.DataFrame(splitted_data, columns=[\"Text\"])\n",
    "    data_c_labeled = splitted_data.assign(Negative=[0 for i in range(len(splitted_data))])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d512f6f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.058716Z",
     "iopub.status.idle": "2021-10-20T14:34:55.059206Z",
     "shell.execute_reply": "2021-10-20T14:34:55.058974Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.058948Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_c_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ee2f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 2.3.1.4 Data - D --> Data H\n",
    "More data, this time a lists of negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693632c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.061592Z",
     "iopub.status.idle": "2021-10-20T14:34:55.062110Z",
     "shell.execute_reply": "2021-10-20T14:34:55.061848Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.061824Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_d.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d1159",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.064002Z",
     "iopub.status.idle": "2021-10-20T14:34:55.065091Z",
     "shell.execute_reply": "2021-10-20T14:34:55.064802Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.064773Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_d_labeled = data_d.rename(columns={\"words\": \"Text\"})\n",
    "    data_d_labeled = data_d_labeled.assign(Negative=[1 for i in range(len(data_d_labeled))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3811466",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.066463Z",
     "iopub.status.idle": "2021-10-20T14:34:55.067518Z",
     "shell.execute_reply": "2021-10-20T14:34:55.067264Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.067226Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_d_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b204a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.069012Z",
     "iopub.status.idle": "2021-10-20T14:34:55.069867Z",
     "shell.execute_reply": "2021-10-20T14:34:55.069705Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.069684Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_e.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95063cc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.070795Z",
     "iopub.status.idle": "2021-10-20T14:34:55.071783Z",
     "shell.execute_reply": "2021-10-20T14:34:55.071554Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.071525Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_e_labeled = [[i, 1] for i in data_e]\n",
    "    data_e_labeled = pd.DataFrame(data_e_labeled, columns = [\"Text\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7eee2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.073232Z",
     "iopub.status.idle": "2021-10-20T14:34:55.073576Z",
     "shell.execute_reply": "2021-10-20T14:34:55.073428Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.073406Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_e_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563f8e7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.074966Z",
     "iopub.status.idle": "2021-10-20T14:34:55.075608Z",
     "shell.execute_reply": "2021-10-20T14:34:55.075451Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.075431Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_f.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd32f0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.076406Z",
     "iopub.status.idle": "2021-10-20T14:34:55.077251Z",
     "shell.execute_reply": "2021-10-20T14:34:55.077054Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.077027Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_f_labeled = data_f.iloc[7]\n",
    "    data_f_labeled = [[i,1] for i in data_f_labeled[0].split(\",\")]\n",
    "    data_f_labeled = pd.DataFrame(data_f_labeled, columns = [\"Text\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17329c3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.078005Z",
     "iopub.status.idle": "2021-10-20T14:34:55.078609Z",
     "shell.execute_reply": "2021-10-20T14:34:55.078420Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.078394Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_f_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd950b0b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.079752Z",
     "iopub.status.idle": "2021-10-20T14:34:55.080104Z",
     "shell.execute_reply": "2021-10-20T14:34:55.079947Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.079924Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_g.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449eb5e2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.081554Z",
     "iopub.status.idle": "2021-10-20T14:34:55.081892Z",
     "shell.execute_reply": "2021-10-20T14:34:55.081727Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.081705Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_g_labeled = data_g.iloc[6]\n",
    "    data_g_labeled = [[i,1] for i in data_g_labeled[0].split(\",\")]\n",
    "    data_g_labeled = pd.DataFrame(data_g_labeled, columns = [\"Text\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9e11d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.082975Z",
     "iopub.status.idle": "2021-10-20T14:34:55.083301Z",
     "shell.execute_reply": "2021-10-20T14:34:55.083149Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.083127Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_g_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dcd59",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.084799Z",
     "iopub.status.idle": "2021-10-20T14:34:55.085729Z",
     "shell.execute_reply": "2021-10-20T14:34:55.085431Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.085401Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_h.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120506a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.086772Z",
     "iopub.status.idle": "2021-10-20T14:34:55.087319Z",
     "shell.execute_reply": "2021-10-20T14:34:55.087156Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.087135Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_h_labeled = data_h.iloc[:, 0]\n",
    "    data_h_labeled = [[i, 1] for i in data_h_labeled]\n",
    "    data_h_labeled = pd.DataFrame(data_h_labeled, columns = [\"Text\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd4f41",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.088314Z",
     "iopub.status.idle": "2021-10-20T14:34:55.088697Z",
     "shell.execute_reply": "2021-10-20T14:34:55.088567Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.088552Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    display(data_h_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0266f6e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 3. Cleaning the data futher for our EDA\n",
    "Concating all the data from all different sources into 1 uniformed data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f566bd4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.089506Z",
     "iopub.status.idle": "2021-10-20T14:34:55.089951Z",
     "shell.execute_reply": "2021-10-20T14:34:55.089793Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.089777Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_dis = pd.DataFrame([len(data_a_labeled[data_a_labeled[\"Negative\"]==1]), len(data_a_labeled[data_a_labeled[\"Negative\"]==0])]+[len(data_b_labeled[data_b_labeled[\"Negative\"]==1]), len(data_b_labeled[data_b_labeled[\"Negative\"]==0])]+[len(data_c_labeled[data_c_labeled[\"Negative\"]==1]), len(data_c_labeled[data_c_labeled[\"Negative\"]==0])]+[len(data_d_labeled[data_d_labeled[\"Negative\"]==1]), len(data_d_labeled[data_d_labeled[\"Negative\"]==0])]+[len(data_e_labeled[data_e_labeled[\"Negative\"]==1]), len(data_e_labeled[data_e_labeled[\"Negative\"]==0])]+[len(data_f_labeled[data_f_labeled[\"Negative\"]==1]), len(data_f_labeled[data_f_labeled[\"Negative\"]==0])]+[len(data_g_labeled[data_g_labeled[\"Negative\"]==1]), len(data_g_labeled[data_g_labeled[\"Negative\"]==0])]+[len(data_h_labeled[data_h_labeled[\"Negative\"]==1]), len(data_h_labeled[data_h_labeled[\"Negative\"]==0])])\n",
    "    plt_x_axis = []\n",
    "    color_axis = []\n",
    "    [[color_axis.append(\"Red\"), color_axis.append(\"Green\"), plt_x_axis.append(\"DB \"+chr(65+i)+\"-\"),plt_x_axis.append(\"DB \"+chr(65+i)+\"+\")] for i in range(int(len(data_dis[0])/2))]\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.bar(plt_x_axis, data_dis[0], width=1, log=True, color=color_axis)\n",
    "    plt.title('Distribution of database size by negative(-) and positive(+) context')\n",
    "    plt.ylabel(\"Cells count\")\n",
    "    plt.xlabel(\"Database\")\n",
    "    print()\n",
    "else:\n",
    "    color_axis = [\"Red\", \"Green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a312983",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.090696Z",
     "iopub.status.idle": "2021-10-20T14:34:55.091171Z",
     "shell.execute_reply": "2021-10-20T14:34:55.090994Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.090970Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    data_unfinished = pd.concat([data_a_labeled,data_b_labeled,data_c_labeled, data_d_labeled, data_e_labeled, data_f_labeled, data_g_labeled, data_h_labeled], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84dd66a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.092031Z",
     "iopub.status.idle": "2021-10-20T14:34:55.092415Z",
     "shell.execute_reply": "2021-10-20T14:34:55.092286Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.092270Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    neg = len(data_unfinished[data_unfinished[\"Negative\"]==1])\n",
    "    pos = len(data_unfinished[data_unfinished[\"Negative\"]==0])\n",
    "else:\n",
    "    neg = len(balanced[balanced[\"Negative\"]==1])\n",
    "    pos = len(balanced[balanced[\"Negative\"]==0])\n",
    "\n",
    "ratio = min(neg, pos)/max(neg, pos)\n",
    "\n",
    "plt.bar([\"Negative\",\"Positive\"], [neg, pos], width=0.9, color=color_axis)\n",
    "plt.title('Combined distribution of combined database size by negative(-) and positive(+) context')\n",
    "plt.ylabel(\"Cells count\")\n",
    "plt.xlabel(\"Ratio:  1 to {}\".format(round(ratio,3)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e7bee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.1 There are, unsuprisingly due to our data sources, more positive sentences than offensive. \n",
    "\n",
    "We can, by design, split the data to have the same number of positive and negative rows.\n",
    "We will also shuffel the rows to have a uniformed data spread along the sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b391840b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e6807",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.093195Z",
     "iopub.status.idle": "2021-10-20T14:34:55.093583Z",
     "shell.execute_reply": "2021-10-20T14:34:55.093452Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.093437Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable this to have 50%/50% of negative and positive\n",
    "split_data = True\n",
    "if SHORT_RUN == True:\n",
    "    split_data = False\n",
    "if LONG_RUN == True:\n",
    "    split_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e743da3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.094427Z",
     "iopub.status.idle": "2021-10-20T14:34:55.094816Z",
     "shell.execute_reply": "2021-10-20T14:34:55.094680Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.094665Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    if split_data:\n",
    "        if neg>pos:\n",
    "            negative_finished = data_unfinished[data_unfinished[\"Negative\"]==1].sample(frac = ratio)\n",
    "            positive_finished = data_unfinished[data_unfinished[\"Negative\"]==0]\n",
    "        else:\n",
    "            positive_finished = data_unfinished[data_unfinished[\"Negative\"]==0].sample(frac = ratio)\n",
    "            negative_finished = data_unfinished[data_unfinished[\"Negative\"]==1]\n",
    "\n",
    "        # Concat 50/50 datasets \n",
    "        balanced = pd.concat([positive_finished,negative_finished], axis=0)\n",
    "        # Shuffle the rows \n",
    "        balanced = balanced.sample(frac = 1).reset_index(drop=True)\n",
    "        data_unbalanced = data_unfinished.sample(frac = 1).reset_index(drop=True)\n",
    "    else:\n",
    "        data_unbalanced = data_unfinished.sample(frac = 1).reset_index(drop=True)\n",
    "        balanced = data_unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce92551",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.095569Z",
     "iopub.status.idle": "2021-10-20T14:34:55.095980Z",
     "shell.execute_reply": "2021-10-20T14:34:55.095811Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.095796Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(balanced[balanced[\"Negative\"]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2479e62",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.096860Z",
     "iopub.status.idle": "2021-10-20T14:34:55.097288Z",
     "shell.execute_reply": "2021-10-20T14:34:55.097155Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.097139Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(balanced[balanced[\"Negative\"]==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3fa33e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Generalize our data so that the NLP model behaves as accurate as possible\n",
    "\n",
    "Machines are better at understanding numbers that actual text passed on as tokens.\n",
    "Therefore, to overcome this problem, we: \n",
    "\n",
    "1. Lowercase the words. In NLP, models treat words like Dog and dog differently, even if they are the same.\n",
    "2. Remove punctuations. Punctuations are the marks in English like commas, hyphens, full stops, etc. These are important for English grammar but not for text analysis.\n",
    "3. Expand contractions. Contractions are the shortened versions of words like don’t for do not. We need to expand these contractions for a better analysis of text. \n",
    "4. Removed some special characters, such as emojis, from our database. We do so if we are certian they hold no special meaning.\n",
    "5. Remove STOP WORDs, words that generally do not add any information at all. Such as \"The\". \n",
    "6. Pass them through Tokenizer that gives the fine tunning and seperation between one word to another. (More in the next reading block). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302f2fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:34:55.100625Z",
     "iopub.status.busy": "2021-10-20T14:34:55.100317Z",
     "iopub.status.idle": "2021-10-20T14:34:55.138357Z",
     "shell.execute_reply": "2021-10-20T14:34:55.136082Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.100599Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def emojis() -> str:\n",
    "    page = get(\"https://www.unicode.org/Public/UCD/latest/ucd/emoji/emoji-data.txt\") # Download the most updated codes. \n",
    "    lines = page.text.split(\"\\n\")\n",
    "\n",
    "    blacklist = [ # blacklist of element who are not really emojis\n",
    "        \"number sign\",\n",
    "        \"asterisk\",\n",
    "        \"digit zero..digit nine\",\n",
    "        \"copyright\",\n",
    "        \"registered\",\n",
    "        \"double exclamation mark\",\n",
    "        \"exclamation question mark\",\n",
    "        \"trade mark\",\n",
    "        \"information\"\n",
    "    ]\n",
    "    \n",
    "    unicodes = []\n",
    "    extendedEmoji = {}\n",
    "    for line in lines: # check all lines\n",
    "        if not line.startswith(\"#\") and len(line) > 0: # ignores comment lines and blank lines\n",
    "            if line.split(')')[1].strip() not in blacklist: # check if the emoji isn't in the blacklist\n",
    "                temp = f\"{line.split(';')[0]}\".strip() # recovery of the first column\n",
    "                if \"..\" in temp: # if it is a \"list\" of emojis, adding to a dict\n",
    "                    extendedEmoji[temp.split(\"..\")[0]] = temp.split(\"..\")[1]\n",
    "                else:\n",
    "                    unicodes.append(temp)\n",
    "    unicodes = list(set(unicodes) - {\"\"}) # removal of duplicates and especially of extra spaces\n",
    "\n",
    "    def _uChar(string: str): # choice between \\u and \\U in addition of the \"0\" to complete the code\n",
    "        stringLen = len(string)\n",
    "        if stringLen > 7: # Can't be more than 7 anyways\n",
    "            raise Exception(f\"{string} is too long! ({stringLen})\")\n",
    "        u, totalLong = \"U\", 7 # Should be 7 characters long if it is a capital U\n",
    "        if stringLen < 4: # 4 characters long if smaller than 4\n",
    "            u, totalLong = \"u\", 4 # Should be 4 characters long if it is a lowercase u\n",
    "        resultat = \"\"\n",
    "        while len(f\"{resultat}{string}\") <= totalLong: # Adding the 0\n",
    "            resultat += \"0\"\n",
    "        return f\"\\{u}{resultat}\" # Return the right \"U\" with the right number of 0\n",
    "\n",
    "    for i in range(0, len(unicodes)): # add unicode syntax to the list\n",
    "        unicodes[i] = f\"{_uChar(unicodes[i])}{unicodes[i]}\"\n",
    "    \n",
    "    for mot in extendedEmoji.items(): # add unicode syntax to the dict\n",
    "        extendedEmoji[mot[0]] = f\"{_uChar(mot[1])}{mot[1]}\"\n",
    "        temp = f\"{_uChar(mot[0])}{mot[0]}-{extendedEmoji[mot[0]]}\"\n",
    "        if temp not in unicodes: # if not already in the list\n",
    "            unicodes.append(temp) # add the item to the list\n",
    "\n",
    "    resultat = \"[\"\n",
    "    for code in unicodes: # conversion of the list into a string with | to separate all the emojis\n",
    "        resultat += f\"{code}|\"\n",
    "\n",
    "    return f\"{resultat[:-1]}]+\"\n",
    "\n",
    "\n",
    "all_emojis = emojis()\n",
    "\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "print(\"\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7a5e4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.139471Z",
     "iopub.status.idle": "2021-10-20T14:34:55.139935Z",
     "shell.execute_reply": "2021-10-20T14:34:55.139767Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.139748Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    balanced[\"Text\"] = balanced[\"Text\"].astype(str)\n",
    "    balanced[\"Text\"] = balanced[\"Text\"].apply(lambda x:expand_contractions(x))\n",
    "    balanced['Text'] = balanced['Text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "    balanced['Text'] = balanced['Text'].apply(lambda x: x.lower())\n",
    "    balanced['Text'] = balanced['Text'].apply(lambda x: re.sub(all_emojis, '', x, flags = re.UNICODE))\n",
    "    balanced[\"Text\"] = balanced[\"Text\"].str.replace('  ', ' ', regex=False)\n",
    "    balanced = balanced[(balanced[[\"Text\"]] != \"\").all(axis=1)]\n",
    "    display(display(balanced.tail()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b87c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.3 Eventually we will tokenize the data with the same common tokenizer.\n",
    "\n",
    "To give a sense of generalization to the data and make it even more uniformed we add an optional feature to transform the data once again with tokenizer.\n",
    "We chose to use NLTK regex as a our final tokenizer and not other due to its speed.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/505/1*yPuBXiIUgoLLzmmE_SvSNg.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Alterntaive option is to use Spacy as a our final tokenizer and not NLTK. This is because while NLTK tokenizers can have a better taylor made solutions, Spacy tokenizer has a better \"Single point solution\" that generally suits all text sources.\n",
    "\n",
    "It is important to note that usually any kind of generalization can reduce the accuracy of our model. In other words, our method will not provide the best reulsts but because the field of tokenization can have a whole notebook of itself we do not want to waste major time on that. Our point here is to enable high flexability for our datasets. Anu user might choose to add or remove some of sets in the future and will not need to customize a lot of the code. \n",
    "\n",
    "Also, as a side note, for dataset \"A\" we used NLTK tweeter custom made tokenizer. For dataset \"B\" we used the sentences NLTK tokenizer that provides faster tokenization.\n",
    "\n",
    "Because of the Spacy DependencyParser, the operation takes some time. If you run it, let the computer work for a while you drink a cup of coffee a refresh ;)\n",
    "\n",
    "[Read more about Spacy here](https://spacy.io/)\n",
    "\n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/2b4127b0cda5f6163af39732b81c233df8d3c822/819d1/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc1f4e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedff8ee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.140690Z",
     "iopub.status.idle": "2021-10-20T14:34:55.141405Z",
     "shell.execute_reply": "2021-10-20T14:34:55.141197Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.141169Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spacy_enabled = True\n",
    "nltk_regex = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e418c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.142618Z",
     "iopub.status.idle": "2021-10-20T14:34:55.143088Z",
     "shell.execute_reply": "2021-10-20T14:34:55.142847Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.142823Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    if spacy_enabled:\n",
    "        try:\n",
    "            sp = spacy.load('en_core_web_sm')\n",
    "        except:\n",
    "            !python -m spacy download en_core_web_sm\n",
    "\n",
    "        sp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        \n",
    "        # Lemmatization with stopwords removal\n",
    "        balanced['sentences']=balanced['Text'].apply(lambda x: ' '.join([token.lemma_ for token in list(sp(x)) if (token.is_stop==False)]))\n",
    "        \n",
    "    else:\n",
    "        balanced['sentences']=balanced['Text']\n",
    "        \n",
    "    \n",
    "    if nltk_regex:\n",
    "        ws_tokenize = WhitespaceTokenizer()\n",
    "        balanced['sentences'] = balanced['sentences'].apply(ws_tokenize.tokenize)\n",
    "        balanced['sentences'] = balanced['sentences'].apply(\" \".join)\n",
    "        \n",
    "    balanced = balanced[(balanced[[\"sentences\"]] != \"\").all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74b09f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.4 Weight per word\n",
    "\n",
    "\n",
    "We can also give weight, per word, for the sentence meaning. \n",
    "\n",
    "We must surely understand by now that \"Fuck\", a 1 word curse said alone, clearly has a negative meaning.\n",
    "While other sentences, such as: \"What the fuck just happend\", has slighly less negative meaning.\n",
    "\n",
    "Lets try to give these sentences weight by the inverse of the number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa79189",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.145060Z",
     "iopub.status.idle": "2021-10-20T14:34:55.145538Z",
     "shell.execute_reply": "2021-10-20T14:34:55.145304Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.145279Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    # We will seperate by the word counts\n",
    "    pattern = re.compile(r'\\w+')\n",
    "    balanced['Number of words'] = balanced['sentences'].apply(lambda x: max(1, len(pattern.findall(x))))\n",
    "    balanced['Weight per word'] = balanced['Number of words'].apply(lambda x: 1/x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f05f95",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.5 Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b2d42",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.147291Z",
     "iopub.status.idle": "2021-10-20T14:34:55.147986Z",
     "shell.execute_reply": "2021-10-20T14:34:55.147734Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.147708Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    l = balanced[\"sentences\"]\n",
    "    w = balanced[\"Number of words\"]\n",
    "\n",
    "    x = [(len(i[0])-i[1]+1)/i[1] for i in zip(l,w)] \n",
    "\n",
    "    balanced = balanced.assign(AverageLength=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a16e4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.149193Z",
     "iopub.status.idle": "2021-10-20T14:34:55.149792Z",
     "shell.execute_reply": "2021-10-20T14:34:55.149572Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.149547Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dadef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.6 Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a83b3d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.151002Z",
     "iopub.status.idle": "2021-10-20T14:34:55.151620Z",
     "shell.execute_reply": "2021-10-20T14:34:55.151376Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.151352Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    prcntl = np.percentile(balanced[\"Number of words\"], 98)\n",
    "    balanced = balanced[balanced[\"Number of words\"]<=prcntl]\n",
    "\n",
    "    prcntl = np.percentile(balanced[\"AverageLength\"], 98)\n",
    "    balanced = balanced[(balanced[\"AverageLength\"]<=prcntl) & (balanced[\"AverageLength\"]>=1.5)].reset_index(drop=True) \n",
    "\n",
    "    display(balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b92fc9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.7 Saving the DataBase as a CSV file\n",
    "<img src=\"https://media.istockphoto.com/vectors/linear-check-mark-icon-like-tick-and-cross-vector-id1019902906?k=20&m=1019902906&s=612x612&w=0&h=EMsD1KlJqPKA770M8HW7BtDLno6FKkzi7agvJWgYYNU=\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79819737",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.152805Z",
     "iopub.status.idle": "2021-10-20T14:34:55.153424Z",
     "shell.execute_reply": "2021-10-20T14:34:55.153201Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.153176Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_database = True\n",
    "if SHORT_RUN == True:\n",
    "    save_database = False\n",
    "if LONG_RUN == True:\n",
    "    save_database = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f4a2c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.154427Z",
     "iopub.status.idle": "2021-10-20T14:34:55.155401Z",
     "shell.execute_reply": "2021-10-20T14:34:55.155157Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.155129Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if activate_db_filtering:\n",
    "    if save_database:\n",
    "        balanced.to_csv('output\\\\database\\\\balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e754ff5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9oZVM2d1JTSFZNblZoZ1RMWEJ5Yk1URDd6ZE84RjJrcUdwSnZFZklkWkxuRFRiRmtKNGFBaWJKSUFnSm9xdmFjU1FlU0hzU0JRemZKUFNBQ2JLZWFkS3cvNjQw?x-oss-process=image/format,png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683871c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 4. EDA\n",
    "\n",
    "## 4.1 More library imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f50b8ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.156558Z",
     "iopub.status.idle": "2021-10-20T14:34:55.157253Z",
     "shell.execute_reply": "2021-10-20T14:34:55.157001Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.156971Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Datastructures \n",
    "import numpy as np\n",
    "\n",
    "# Save results\n",
    "from joblib import dump, load\n",
    "\n",
    "# Plots and Analysis\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels\n",
    "from collections import Counter\n",
    "\n",
    "# Model evaluation and results\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# BoW\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Temp var to save results \n",
    "result = {}\n",
    "time_dict = {}\n",
    "\n",
    "save = True\n",
    "model_load = True\n",
    "bow_run = True\n",
    "\n",
    "if SHORT_RUN:\n",
    "    model_load = True\n",
    "    bow_run = False\n",
    "if LONG_RUN:\n",
    "    model_load = False\n",
    "    bow_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cf686",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Database information\n",
    "\n",
    "### 4.2.1 DB Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac76f3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.158546Z",
     "iopub.status.idle": "2021-10-20T14:34:55.158870Z",
     "shell.execute_reply": "2021-10-20T14:34:55.158721Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.158706Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "balanced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3ed9c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2.2 Distribution of number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d456d0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.159989Z",
     "iopub.status.idle": "2021-10-20T14:34:55.160499Z",
     "shell.execute_reply": "2021-10-20T14:34:55.160365Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.160348Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = balanced[\"Number of words\"]\n",
    "y = balanced[\"Negative\"]\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.hist(x, bins=25)\n",
    "plt.title('Distribution of number of words per cell')\n",
    "plt.ylabel(\"counts in bin\")\n",
    "plt.xlabel(\"Number of words\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27a2177",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2.3 Distribution of context vs. number of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba35667",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.161441Z",
     "iopub.status.idle": "2021-10-20T14:34:55.162017Z",
     "shell.execute_reply": "2021-10-20T14:34:55.161844Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.161827Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_list = [[j, i] for j, i in sorted(zip(x, y))]\n",
    "x = [i[0] for i in sorted_list]\n",
    "y = [i[1] for i in sorted_list]\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.hist2d(x, y, bins=[40, 2], range=[[0, 41], [0,1]], cmap='viridis')\n",
    "plt.ylabel(\"Positive        |        Negative\")\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks([])\n",
    "plt.xticks(np.arange(1,41))\n",
    "plt.xlabel(\"Number of words in a sentence\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('DB counts in bin')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef4339",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Normalizing the plot above gives us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89a33f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.163338Z",
     "iopub.status.idle": "2021-10-20T14:34:55.163651Z",
     "shell.execute_reply": "2021-10-20T14:34:55.163503Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.163488Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "data = balanced\n",
    "data[\"Positive Percentage\"] = balanced['Negative'].apply(lambda x: np.abs(1-x))\n",
    "g = sns.kdeplot(data=data, x=\"Number of words\", hue=\"Positive Percentage\", multiple=\"fill\", legend=False)\n",
    "plt.ylabel(\"Positive %       |        Negative %\")\n",
    "plt.xlim(1, min(900, max(data[\"Number of words\"])))\n",
    "plt.xlabel(\"Number of words in a sentence\")\n",
    "plt.xticks(np.arange(25, min(900, max(data[\"Number of words\"])), step=25))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b711655",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.164769Z",
     "iopub.status.idle": "2021-10-20T14:34:55.165143Z",
     "shell.execute_reply": "2021-10-20T14:34:55.165008Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.164991Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "labeled_hue = [\"Negative\" if i==1 else \"Positive\" for i in y] \n",
    "plt.figure(figsize=(18, 4))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.violinplot(x=[\"\" for i in x], y=x, hue=labeled_hue, split=True,\n",
    "               scale=\"area\", linewidth=0, palette={\"Negative\": \"r\", \"Positive\": \"g\"})\n",
    "plt.ylabel(\"Number of words in a sentence\")\n",
    "plt.xlabel(\"Number of examples in our database\")\n",
    "plt.title('Distribution')\n",
    "plt.ylim(0, max(x))\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ffaaa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2.4 Distribution of average words legnth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26218b9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.166476Z",
     "iopub.status.idle": "2021-10-20T14:34:55.166978Z",
     "shell.execute_reply": "2021-10-20T14:34:55.166797Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.166779Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = balanced[\"Negative\"]\n",
    "x = balanced[\"AverageLength\"] \n",
    "plt_max_range = int(np.percentile(x, 99)) \n",
    "plt_min_range = int(np.percentile(x, 1))\n",
    "xRangeArr = np.arange(plt_min_range, plt_max_range, step=0.2)\n",
    "\n",
    "xx = [i for i in x if ((i>= plt_min_range) and (i<= plt_max_range))]\n",
    "\n",
    "mu, std = stats.norm.fit(xx) \n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.hist(x, bins=len(xRangeArr), density=True, range=[plt_min_range, plt_max_range], alpha=0.5)\n",
    "plt.title('Distribution of average word length in a sentence')\n",
    "plt.ylabel(\"Density of bin\")\n",
    "plt.xlabel(\"Average word length\")\n",
    "plt.xticks(xRangeArr)\n",
    "xmin, xmax = xRangeArr[0], xRangeArr[-1]\n",
    "plt.xlim(xmin, xmax)\n",
    "xx = np.linspace(xmin, xmax, 100)\n",
    "pp = stats.norm.pdf(xx, mu, std)\n",
    "plt.plot(xx, pp, linewidth=3, color=\"red\")\n",
    "title = \"Normal distribution of values: mean {:.2f} and std {:.2f}\".format(mu, std)\n",
    "plt.title(title, color=\"red\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef03e79",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2.5 Distribution of context vs. average words legnth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803594ad",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.167850Z",
     "iopub.status.idle": "2021-10-20T14:34:55.168301Z",
     "shell.execute_reply": "2021-10-20T14:34:55.168167Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.168150Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.hist2d(x, y, bins=[40, 2], range=[[xRangeArr[0], xRangeArr[-1]], [0,1]], cmap=plt.cm.jet)\n",
    "plt.ylabel(\"Positive Sentence        |        Negative Sentence\")\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks([])\n",
    "plt.xticks(xRangeArr)\n",
    "plt.xlabel(\"Average word length in a sentence\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('DB counts in bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be2278",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Normalizing the plot above gives us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a073583",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.169298Z",
     "iopub.status.idle": "2021-10-20T14:34:55.169919Z",
     "shell.execute_reply": "2021-10-20T14:34:55.169733Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.169715Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "sns.kdeplot(data=data, x=\"AverageLength\", hue=\"Positive Percentage\", multiple=\"fill\", legend=False)\n",
    "plt.ylabel(\"Positive %       |        Negative %\")\n",
    "plt.xlim(max(1, min(balanced[\"AverageLength\"])), min(15, max(balanced[\"AverageLength\"])))\n",
    "plt.xticks(np.arange(max(1, min(balanced[\"AverageLength\"])), min(15, max(balanced[\"AverageLength\"])), step=1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba784e73",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2.6 Plotting the appearance frequency - most of our tokens are quite unique (Score of almost 1)\n",
    "Note: Below is already implementation of BoW with TFIDF. Explanation for the code can be found in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16feca19",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.170717Z",
     "iopub.status.idle": "2021-10-20T14:34:55.171362Z",
     "shell.execute_reply": "2021-10-20T14:34:55.171194Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.171174Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = [\"Positive\", \"Negative\"]\n",
    "\n",
    "if bow_run:\n",
    "    start = time.time()\n",
    "    X = balanced[\"sentences\"]\n",
    "    bag_of_words = TfidfVectorizer(stop_words=\"english\", use_idf=True)\n",
    "    X = bag_of_words.fit_transform(X) \n",
    "    end = time.time()\n",
    "    time_dict[\"BoW_construction\"] = end - start\n",
    "    \n",
    "    if save:\n",
    "        dump(bag_of_words, \"saved_runs\\\\tfidf\\\\bag_of_words.joblib\")\n",
    "        dump(X, \"saved_runs\\\\tfidf\\\\X.joblib\")\n",
    "elif model_load:\n",
    "    bag_of_words = load(input_path+'saved_runs\\\\tfidf\\\\bag_of_words.joblib')\n",
    "    X = load(input_path+'saved_runs\\\\tfidf\\\\X.joblib')\n",
    "\n",
    "\n",
    "\n",
    "idf_vector = bag_of_words.idf_ \n",
    "idf_vector = idf_vector / np.max(idf_vector)\n",
    "counts, bins = np.histogram(idf_vector)\n",
    "\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(20, 4))\n",
    "plt.hist(bins[:-1], bins=bins, weights=counts, color=\"Orange\", log=True)\n",
    "plt.title('Histogram of words uniqueness')\n",
    "plt.xlabel('Normalzed weight per word (uniqueness index)')\n",
    "plt.ylabel('Number of unique words with that weight')\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(0, 1, step=0.05))\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4575e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2.7 Early conclusions\n",
    "\n",
    "1. There are more possible positive words available than negative words(or sentences) in our database\n",
    "2. We have more \"short\" sentences (or even single word) than long\n",
    "3. Single word cells are usually negative in our case\n",
    "4. Distribution of words legnth looks like normal distribution\n",
    "5. Looks like 2-4 letters words or more common to be offensive where 6-8 letters are more common to be safe\n",
    "6. The total amount of unique tokens is higher than the number of samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd7b38",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.172230Z",
     "iopub.status.idle": "2021-10-20T14:34:55.172834Z",
     "shell.execute_reply": "2021-10-20T14:34:55.172682Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.172665Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting_data = balanced[balanced[\"Number of words\"]<=20]\n",
    "plotting_data = plotting_data.sort_values(by=['Number of words'])\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "sns.lmplot(x=\"AverageLength\", y=\"Negative\", hue=\"Number of words\", x_bins = 20, logistic=True, data=plotting_data, \n",
    "               palette=\"rocket\", legend=True, height=20, aspect=20/4)\n",
    "plt.legend(title='Color Legend', bbox_to_anchor=(1.05, 1), fontsize = 40, title_fontsize=40, ncol=4, loc='upper left')\n",
    "plt.xlabel(\"AverageLength\", fontsize=40)\n",
    "plt.ylabel(\"Negative\", fontsize=40)\n",
    "plt.title(\"Summary\", fontsize=40)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1692b68b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4.3 Word Embeddings and Language Models\n",
    "\n",
    "Expressing power of notations used to represent a vocabulary of a language has been a great deal of interest in the field of linguistics. Languages in practice have semantic ambiguity. \n",
    "\n",
    "“John kissed his wife, and so did Sam”, \"These are some two very beautiful apples on that woman\". \n",
    "Sam kissed John’s wife or his own? These ambiguities must be handled in order to represent information in true form.\n",
    "\n",
    "### 4.3.1 What are Word Embeddings?\n",
    "Word Embeddings are the texts converted into numbers. There may be different numerical representations of the same text.\n",
    "Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression, etc. in broad terms.\n",
    "\n",
    "### 4.3.2 Language Models \n",
    "There are several models we shall discuss while trying to solve our problem. Some of these models, like the BoW (Bag Of Words) model, we will train ourselves. \n",
    "Others, such as the BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "In their essense, language models (LM) are the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions. \n",
    "\n",
    "We devided our work to 2 kinds of Word Embeddings, \n",
    "1. Frequency based embeddings (Such as the Count vector and TF-IDF)\n",
    "2. Pre-trained word embeddings (Such as Word2Vec, GloVe, BERT, fastText)\n",
    "\n",
    "\n",
    "#### 4.3.3 More definitions\n",
    "\n",
    "##### 4.3.3.1 N Grams\n",
    "In addition, sentences can be splitted into \"N grams\", which basically means how many word tokens we take together.\n",
    "\n",
    "<img src=\"https://studymachinelearning.com/wp-content/uploads/2019/09/n_gram_ex.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Selecting the N grams range to (1,2), will provide us the following output:\n",
    "\n",
    "('Bi-grams are cool!') == (['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\n",
    "\n",
    "\n",
    "##### 4.3.3.2 SVD \n",
    "\n",
    "\n",
    "High dimensional problem are solved by PCA or by its generalized version SVD. SVD (singular value decomposition) is a generalization of the eigen-decomposition of a positive semi-definite normal matrix to any matrix via an extension of the polar decomposition. It gives the best rank-k approximation of the original data. Let original data X be of dimension m x n. The singular value decomposition will break this into best rank approximation capturing information from most relevant to least relevant ones\n",
    "\n",
    "The SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix. By creating an SVD we, sometimes, can decrease the amount of features we need to train a model to anticipate are results while keeping high accuracy.\n",
    "In our case our issue is that the dimensions of the matrix change very often (new words are added very frequently and corpus might change in size).\n",
    "Also, the matrix is extremely sparse since most words do not cooccur. There are also many features so that the reduction is somehow limmitted. \n",
    "The cost to perform SVD is Quadratid (i.e. to perform SVD). \n",
    "\n",
    "The elements of Sigma, namely σ₁, σ₂,… σₙ are the non-zero singular values of our Sparse Matrix \"X\". The higher their value the more effect \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Singular_value_decomposition.gif/280px-Singular_value_decomposition.gif\" alt=\"Drawing\" style=\"width: 300px;\"/> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Singular_value_decomposition_visualisation.svg/220px-Singular_value_decomposition_visualisation.svg.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*37a_i1t1tDxDYT3ZI6Yn8w.gif\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### 4.3.3.4 LSI \n",
    "or Latent semantic indexing is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.\n",
    "Efficient LSI( algorithms only compute the first k singular values and term and document vectors as opposed to computing a full SVD and then truncating it.\n",
    "\n",
    "Note that this rank reduction is essentially the same as doing Principal Component Analysis (PCA) on the matrix A, except that PCA subtracts off the means. PCA loses the sparseness of the A matrix, which can make it infeasible for large lexicons\n",
    "\n",
    "\n",
    "\n",
    "Read more about Rank-reduced singular value decomposition in the following link:\n",
    "[SVD and LSA](https://en.wikipedia.org/wiki/Latent_semantic_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c667fc5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 5. Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d1644",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5.1 Method 1 - Frequency based embeddings (Bag of Words and TfidfVectorizerion)\n",
    "\n",
    "<a href=\"https://ibb.co/4pByhFW\"><img src=\"https://i.ibb.co/DQ3X014/Bo-W-process.png\" alt=\"Bo-W-process\" border=\"0\" /></a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5.1.1 The Bag of Words (BoW) \n",
    "BoW is the simplest form of text representation in numbers. Like the term itself, we can represent a sentence as a bag of words vector (a string of numbers).\n",
    "In tasks in which words are features, the bag-of-words model can be used to create a feature vector when the number of features (words) is not known in advance, with the assumption that their order is not important. Each word is represented by a one-hot vector - a sparse vector in the size of the vocabulary, with 1 in the entry representing the word and 0 in all other entries. The bag-of-words feature vector is the sum of all one-hot vectors of the words, and therefore has a non-zero value for every word that occurred. \n",
    "\n",
    "<a href=\"https://ibb.co/BP8DNxM\"><img src=\"https://i.ibb.co/xsVvH0T/bag-of.png\" alt=\"bag-of\" border=\"0\"></a>\n",
    "\n",
    "In the weighted variation explained below, it is a weighted sum according to frequency or TF-IDF scores\n",
    "\n",
    "#### 5.1.2 TfidfVectorizerion is CountVectorizer (bag of words) with TfidfTransformer.\n",
    "This means we basically fitst count the number of occurrences for all token and later we normalize it according to the frequencies. \n",
    "\n",
    "Term Frequent (Tf - CountVectorizer) is a measure of how frequently a term, t, appears in our dataset.\n",
    "Inverse Document Frequency (idf) is a measure of how important a term is. We need the IDF value because computing just the Tf alone is not sufficient to understand the importance of words. Hence, we see that words like “is”, “this”, “and”, etc., are reduced to values closer to 0 and have little importance; while words like “smart”, “amazing”, etc. are words less frequent, thus with more importance. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5.1.3 An advantage with count vector (and Tfidf):\n",
    "Count Vector and TF-IDF are quite easy to understand and the implementation is fairly fast to deploy.\n",
    "\n",
    "#### 5.1.4 A clear disadvatange with count vector (and Tfidf):\n",
    "Count Vector and TF-IDF do not capture the position in semantics, co-occurrences in the document, etc.\n",
    "In our set of examples we might \n",
    "\n",
    "#### 5.1.5 Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e258b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### 5.1.5.1 BoW words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58026e29",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.174164Z",
     "iopub.status.idle": "2021-10-20T14:34:55.174636Z",
     "shell.execute_reply": "2021-10-20T14:34:55.174403Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.174379Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = balanced['Negative']\n",
    "\n",
    "if bow_run:\n",
    "    start = time.time()\n",
    "    X = balanced[\"sentences\"]\n",
    "    bag_of_words = TfidfVectorizer(stop_words=\"english\", use_idf=True) # INIT the model\n",
    "    X = bag_of_words.fit_transform(X)  # Transform our tokens to freq based sparse matrix \n",
    "    end = time.time()\n",
    "    time_dict[\"BoW_construction\"] = end - start\n",
    "    \n",
    "    if save:\n",
    "        dump(bag_of_words, \"saved_runs\\\\tfidf\\\\bag_of_words.joblib\")\n",
    "        dump(X, \"saved_runs\\\\tfidf\\\\X.joblib\")\n",
    "elif model_load:\n",
    "    bag_of_words = load(input_path+'saved_runs\\\\tfidf\\\\bag_of_words.joblib')\n",
    "    X = load(input_path+'saved_runs\\\\tfidf\\\\X.joblib')\n",
    "# Accuracy, Precision, Recall\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659ea0e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.176326Z",
     "iopub.status.idle": "2021-10-20T14:34:55.176781Z",
     "shell.execute_reply": "2021-10-20T14:34:55.176563Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.176540Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(X_train[0:1000].todense()[:,np.random.randint(0,100,500)]==0, vmin=0, vmax=1, cmap=\"YlGnBu\",cbar=False).set_title('Sample of Sparse Matrix')\n",
    "plt.text(75, 75, 'White represents non zero entries', horizontalalignment='center', verticalalignment='center', size=\"large\", color=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ecc4a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Sparse matrices are those matrices that have the majority of their elements equal to zero. In other words, the sparse matrix can be defined as the matrix that has a greater number of zero elements than the non-zero elements.\n",
    "Storing only the non-zero values and their positions is a common technique in storing sparse data sets and thus avoiding handling a sparse matrix as a dense one which makes excessive use of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27923f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##### 5.1.5.2 Rank Reduction\n",
    "TruncatedSVD\n",
    "[Read more here](https://math.stackexchange.com/questions/2627005/are-reduced-svd-and-truncated-svd-the-same-thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18700db",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.178714Z",
     "iopub.status.idle": "2021-10-20T14:34:55.179053Z",
     "shell.execute_reply": "2021-10-20T14:34:55.178882Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.178859Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "svd_enabled = True\n",
    "n_components_array = [3000] \n",
    "\n",
    "if SHORT_RUN:\n",
    "    svd_enabled = False\n",
    "if LONG_RUN:\n",
    "    svd_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec5dbc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.180222Z",
     "iopub.status.idle": "2021-10-20T14:34:55.180822Z",
     "shell.execute_reply": "2021-10-20T14:34:55.180648Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.180628Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "    if svd_enabled:\n",
    "        for n_components in n_components_array:\n",
    "            start = time.time()\n",
    "            svd = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "            tfidf_lsa_data = svd.fit_transform(X)\n",
    "            Sigma = svd.singular_values_\n",
    "            V_T = svd.components_.T\n",
    "\n",
    "            end = time.time()\n",
    "            time_dict[\"BoW_svd_{}_construction\".format(n_components)] = end - start\n",
    "\n",
    "            dump(svd, \"saved_runs\\\\tfidf\\\\svd_{}_TruncatedSVD.joblib\".format(n_components))\n",
    "            dump(tfidf_lsa_data, \"saved_runs\\\\tfidf\\\\svd_{}_arpack_tfidf_isa_data.joblib\".format(n_components))\n",
    "            dump(Sigma, \"saved_runs\\\\tfidf\\\\svd_{}_arpack_sigma.joblib\".format(n_components))\n",
    "            dump(V_T, \"saved_runs\\\\tfidf\\\\svd_{}_arpack_V_T.joblib\".format(n_components))\n",
    "            svd_bow_X_train,svd_bow_X_test,y_train,y_test=train_test_split(tfidf_lsa_data,y,test_size=0.2,shuffle=False)\n",
    "    \n",
    "    elif model_load:\n",
    "        n_components = n_components_array[-1]\n",
    "        tfidf_lsa_data = load(input_path+\"saved_runs\\\\tfidf\\\\svd_{}_arpack_tfidf_isa_data.joblib\".format(n_components))\n",
    "        Sigma = load(input_path+\"saved_runs\\\\tfidf\\\\svd_{}_arpack_sigma.joblib\".format(n_components))\n",
    "        V_T = load(input_path+\"saved_runs\\\\tfidf\\\\svd_{}_arpack_V_T.joblib\".format(n_components))\n",
    "        svd_bow_X_train,svd_bow_X_test,y_train,y_test=train_test_split(tfidf_lsa_data,y,test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdaac4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.181821Z",
     "iopub.status.idle": "2021-10-20T14:34:55.182126Z",
     "shell.execute_reply": "2021-10-20T14:34:55.181996Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.181981Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if svd_enabled or model_load:\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    counts, bins = np.histogram(Sigma, bins=int(Sigma.shape[0]/4))\n",
    "    plt.hist(bins[:-1], bins=bins, weights=counts, color=\"Orange\", log=True)\n",
    "    plt.xticks(np.arange(1, np.ceil(np.max(Sigma))+1, step=1))\n",
    "    \n",
    "    plt.xlabel('Singualar vector weight - higher is more meaningful')\n",
    "    plt.ylabel('Number of vectors with that weight')\n",
    "    \n",
    "print(\"DIM Reduction...\\n|{}| ---> |{}|   ~= {}%\".format(X.shape[1], tfidf_lsa_data.shape[1], np.round(tfidf_lsa_data.shape[1]/X.shape[1]*100, 3)))\n",
    "print(\"SIGMA Values...\\nMax|{}| ---> Min|{}|\".format(np.round(max(Sigma), 3), np.round(min(Sigma), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9569b53",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.183427Z",
     "iopub.status.idle": "2021-10-20T14:34:55.183733Z",
     "shell.execute_reply": "2021-10-20T14:34:55.183602Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.183587Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if svd_enabled or model_load:\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    show_first = int(svd.explained_variance_ratio_.shape[0])\n",
    "    plot_x = np.arange(1, svd.explained_variance_ratio_.shape[0]+1, 1)\n",
    "    plt.plot(plot_x[:show_first], svd.explained_variance_ratio_.cumsum()[:show_first])\n",
    "    \n",
    "    plt.fill_between(plot_x[:show_first], svd.explained_variance_ratio_.cumsum()[:show_first], facecolor = \"green\", alpha=0.1)\n",
    "    \n",
    "    plt.plot(plot_x[:show_first], svd.explained_variance_ratio_[:show_first], 'ro-')\n",
    "    plt.legend([\"Cumulative Value\", \"Componenet\"], loc='upper left')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('Componenets (Log based)')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylabel('Proportion of Variance Explained')\n",
    "    plt.xscale(\"log\")\n",
    "    print(\"Least significat vector addition:   {:<25}  %\".format(svd.explained_variance_ratio_[0]))\n",
    "    print(\"Most  significat vector addition:   {:<25}  %\".format(svd.explained_variance_ratio_[-1]))\n",
    "    print(\"Total amount of varian explained:   {:<25}  %\".format(svd.explained_variance_ratio_.cumsum()[-1]))\n",
    "    \n",
    "    log_val = int(np.log10(show_first))\n",
    "    x_column = [svd.explained_variance_ratio_.cumsum()[0]]\n",
    "    [x_column.append(svd.explained_variance_ratio_.cumsum()[10**i]) for i in range(1, log_val+1)]\n",
    "    x_column = np.round(x_column, 5)\n",
    "    \n",
    "    print(\"\\n  \\t\", x_column[0], \" \", end=\"\")\n",
    "    [print(\"------------------------------> {}  \".format(i) ,end=\"\") for i in x_column[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea87771",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Looking at the plot above we can see the clear proportions in logarithmic scale.\n",
    "It appears that roughly each 2 orders of magnitude in the number of dimmensions increases our explained proportion by an order of magnitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0c500",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5.2 Models for evaluating BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090c7ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.1 Intro\n",
    "\n",
    "All of our posible models are based on the idea that some words can have more than 1 meaning. How to decide whether a word has a negative or positive context is not an easy task. While some words are clearly offensive, some may or may not be offensive. Thus the splitting of the data have a critical effect of the learning and validation process. One way to overcome overfitting and reach the best results it is to use cross validation.\n",
    "\n",
    "\n",
    "#### 5.2.1.1 CCCV\n",
    "In our case we have a new, untrained model.\n",
    "We will create our CalibratedClassifierCV. With cv in the parameters as the number of folds.\n",
    "We later fit the model. Because our model is untrained, X and y have to be used for both training and calibration. \n",
    "The way to ensure the data is 'disjoint' is our cross validation: \n",
    "for any given fold, CCCV will split X and y into your training and calibration data, so they do not overlap.\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/Q81RCH3/k-fold.jpg\" alt=\"k-fold\" border=\"0\" /></a>\n",
    "\n",
    "#### 5.2.1.2 Precision\n",
    "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.\n",
    "Our data is labeld \"1\" as offensive. Precision will be our index for the \"Positive\" words labeling.\n",
    "\n",
    "\n",
    "#### 5.2.1.3 Recall \n",
    "Recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. \n",
    "This index is slighly more important as it reflects the ability of a model to find all the relevant cases within a data set.\n",
    "For some extent, if we classify offensive data we would prefer to \"filter\" data that should not have been filtered rather than let some young kid recieve offensive content. \n",
    "\n",
    "#### 5.2.1.4 f1-score (Accuracy) \n",
    "The F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive.\n",
    "The F1 score is the harmonic mean of the precision and recall\n",
    "\n",
    "#### 5.2.1.5 Confusion Matrix \n",
    "The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. These later represent the Recall and Precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0c27c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.185114Z",
     "iopub.status.idle": "2021-10-20T14:34:55.185402Z",
     "shell.execute_reply": "2021-10-20T14:34:55.185272Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.185257Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6381382",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.187104Z",
     "iopub.status.idle": "2021-10-20T14:34:55.187604Z",
     "shell.execute_reply": "2021-10-20T14:34:55.187460Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.187441Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable\n",
    "svm = True\n",
    "bernoulli_bayes = True\n",
    "multinomial_bayes = True\n",
    "complement_bayes = True\n",
    "logistic = True\n",
    "random_forest = True\n",
    "neigbors = True\n",
    "tree = True\n",
    "\n",
    "# Major effect on runtime\n",
    "max_iter_runtime = 10**5  # For LinearSVC\n",
    "tolerance = 10**(-2)      # For LinearSVC\n",
    "\n",
    "neighbors = 10            # K-Neighbors\n",
    "\n",
    "min_samples_split = 2    # Decision Tree\n",
    "min_samples_leaf = 1     # Decision Tree\n",
    "\n",
    "k_fold = 5               # K fold cross validation (CV)\n",
    "\n",
    "nb_epoch=3\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b46f0e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.188509Z",
     "iopub.status.idle": "2021-10-20T14:34:55.189005Z",
     "shell.execute_reply": "2021-10-20T14:34:55.188786Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.188761Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SHORT_RUN == True:\n",
    "    svm = False\n",
    "    bernoulli_bayes = False\n",
    "    multinomial_bayes = False\n",
    "    complement_bayes = False\n",
    "    logistic = False\n",
    "    random_forest = False\n",
    "    neigbors = False\n",
    "    tree = False\n",
    "if LONG_RUN == True:\n",
    "    svm = False\n",
    "    bernoulli_bayes = True\n",
    "    multinomial_bayes = True\n",
    "    complement_bayes = True\n",
    "    logistic = True\n",
    "    random_forest = True\n",
    "    neigbors = True\n",
    "    tree = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d625b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.2 Naive Bayes - Bernoulli, Multinomial, Complement . In our case Multinomial.\n",
    "\n",
    "Naive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem\n",
    "\n",
    "#### 5.2.2.1 Abstract equation\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/YcbnRrL/Naive-bayes.png\" alt=\"Naive-bayes\" border=\"0\" /></a>\n",
    "\n",
    "In plain English, this equation is used to answer the following question. “What is the probability of y (my output variable) given X?\n",
    "\n",
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters.\n",
    "Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "\n",
    "We have added 3 examples of Naive Bayes models that might fit our needs. \n",
    "Bare in mind the Bag Of Words can be repesented as counts, binary and freq and each one of them might have different outcomes. \n",
    "\n",
    "\n",
    "#### 5.2.2.2 Multinomial Naive Bayes: \n",
    "Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification which is our case.\n",
    "Our assumption is that Multinomial Naive Bayes will best fit our needs with our frequency vector\n",
    "\n",
    "#### 5.2.2.3 Bernoulli Naive Bayes: \n",
    "In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence(i.e. a word occurs in a document or not) features are used rather than term frequencies(i.e. frequency of a word in the document)\n",
    "\n",
    "#### 5.2.2.4 Complement Naive Bayes:\n",
    "The Complement Naive Bayes classifier was designed to correct the “severe assumptions” made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.\n",
    "Feel free to read more here: [Rennie et al. (2003)](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f4d9e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.189954Z",
     "iopub.status.idle": "2021-10-20T14:34:55.190408Z",
     "shell.execute_reply": "2021-10-20T14:34:55.190251Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.190226Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# Naive Bayes\n",
    "    if bernoulli_bayes:\n",
    "        start = time.time()\n",
    "        model = BernoulliNB()\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        result[\"BoW_BernoulliNB\"] = score\n",
    "        print(\"Score for bernoulli_bayes classifier with fold={} = {} %\".format(k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_BernoulliNB\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\bernoulli_bayes_calibrated.joblib\")\n",
    "    \n",
    "    if complement_bayes:\n",
    "        start = time.time()\n",
    "        model = ComplementNB()\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        result[\"BoW_ComplementNB\"] = score\n",
    "        print(\"Score for complement_bayes classifier with fold={} = {} %\".format(k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_ComplementNB\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\complement_bayes_calibrated.joblib\")\n",
    "    \n",
    "    if multinomial_bayes:\n",
    "        start = time.time()\n",
    "        model = MultinomialNB()\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        result[\"BoW_MultinomialNB\"] = score\n",
    "        print(\"Score for multinomial_bayes classifier with fold={} = {} %\".format(k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_MultinomialNB\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\multinomial_bayes_calibrated.joblib\")\n",
    "elif model_load:\n",
    "    if multinomial_bayes:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\multinomial_bayes_calibrated.joblib')\n",
    "    elif complement_bayes:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\bernoulli_bayes_calibrated.joblib')\n",
    "    elif bernoulli_bayes:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\bernoulli_bayes_calibrated.joblib')\n",
    "    else:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\multinomial_bayes_calibrated.joblib')\n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede535be",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.191522Z",
     "iopub.status.idle": "2021-10-20T14:34:55.191912Z",
     "shell.execute_reply": "2021-10-20T14:34:55.191713Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.191691Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (multinomial_bayes or complement_bayes or bernoulli_bayes) or load:\n",
    "    predicted = calibrated.predict(X_test)\n",
    "    predicted_prob = calibrated.predict_proba(X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e590b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.3 K-Neighbors classifier\n",
    "\n",
    "The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other\n",
    "\n",
    "As we increase the value of K, that is the number of neihbors, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n",
    "\n",
    "#### 5.2.3.1 Clear disadvantage of KNN\n",
    "If we look mathematically, the simple intuition is to calculate the euclidean distance from point of interest ( of whose class we need to determine) to all the points in training set. Then we take class with majority points. This is called brute force method.\n",
    "For N samples in D dimensions the running time complexity turns out to be O[DN²]. If we have small number of dimensions and training set, this would run in reasonable time. But as the training set size increases, the running time grows quickly.\n",
    "Brute force performs worst when there are large dimensions and large training set.\n",
    "\n",
    "With SKLearn fitting on sparse input always uses brute force method\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/GT7TQ8J/KNN.png\" alt=\"KNN\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af25eb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.193078Z",
     "iopub.status.idle": "2021-10-20T14:34:55.193394Z",
     "shell.execute_reply": "2021-10-20T14:34:55.193253Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.193233Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# K-Neighbors classifier\n",
    "    if neigbors:\n",
    "        start = time.time()\n",
    "                          \n",
    "        model = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        result[\"BoW_KNeighborsClassifier\"] = score\n",
    "        print(\"Score for K-Neighbors classifier with neighbors={}, fold={} > {} %\".format(neighbors, k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_KNeighborsClassifier\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\kneighbors_calibrated.joblib\")\n",
    "elif model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\kneighbors_calibrated.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91c381",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.195365Z",
     "iopub.status.idle": "2021-10-20T14:34:55.196025Z",
     "shell.execute_reply": "2021-10-20T14:34:55.195767Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.195740Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if neigbors or model_load:\n",
    "    predicted = calibrated.predict(X_test)\n",
    "    predicted_prob = calibrated.predict_proba(X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d908451",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.4 Decision tree\n",
    "\n",
    "Each node splits the decision and the more nodes we have, the more accurate the decision tree will be. \n",
    "The last nodes of the decision tree, the leaf, is where the decision is being made.\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/cD0823g/tree-image.png\" alt=\"tree-image\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbb49d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.197072Z",
     "iopub.status.idle": "2021-10-20T14:34:55.197708Z",
     "shell.execute_reply": "2021-10-20T14:34:55.197466Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.197431Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# Decision tree classifier\n",
    "    if tree:\n",
    "        start = time.time()\n",
    "                          \n",
    "        model = DecisionTreeClassifier(min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        result[\"BoW_DecisionTreeClassifier\"] = score\n",
    "        print(\"Score for Decision tree classifier with min_samples_split={}, min_samples_leaf={}, fold={} > {} %\".format(min_samples_split, min_samples_leaf, k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_DecisionTreeClassifier\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\tree_calibrated.joblib\")\n",
    "elif model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\tree_calibrated.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ffe81",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.199255Z",
     "iopub.status.idle": "2021-10-20T14:34:55.199989Z",
     "shell.execute_reply": "2021-10-20T14:34:55.199731Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.199704Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if tree or model_load:\n",
    "    predicted = calibrated.predict(X_test)\n",
    "    predicted_prob = calibrated.predict_proba(X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e2326",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.5 RandomForestClassifier\n",
    "Random forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. What’s the point of this? By relying on a “majority wins” model, it reduces the risk of error from an individual tree.\n",
    "\n",
    "<a href=\"https://ibb.co/JCvZy8G\"><img src=\"https://i.ibb.co/wYStw8P/random-forest.png\" alt=\"random-forest\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e05fc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.201313Z",
     "iopub.status.idle": "2021-10-20T14:34:55.201939Z",
     "shell.execute_reply": "2021-10-20T14:34:55.201699Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.201674Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# RandomForestClassifier\n",
    "    if random_forest:\n",
    "        start = time.time()\n",
    "        model = RandomForestClassifier()\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        result[\"BoW_RandomForest\"] = score\n",
    "        print(\"Score for RandomForestClassifier classifier with fold={} > {} %\".format(k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_RandomForest\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\RandomForestClassifier_calibrated.joblib\")\n",
    "elif model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\RandomForestClassifier_calibrated.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559cfec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.203168Z",
     "iopub.status.idle": "2021-10-20T14:34:55.203789Z",
     "shell.execute_reply": "2021-10-20T14:34:55.203564Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.203539Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if random_forest or model_load:\n",
    "    predicted = calibrated.predict(X_test)\n",
    "    predicted_prob = calibrated.predict_proba(X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1ffd6c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.6 Logistic regression\n",
    "Logistic regression is similar to linear regression but is used to model the probability of a finite number of outcomes, typically two. There are a number of reasons why logistic regression is used over linear regression when modeling probabilities of outcomes.\n",
    "\n",
    "<a href=\"https://ibb.co/YDm6g4G\"><img src=\"https://i.ibb.co/28xwJ0B/Logistic-regression-and-linear-regression.png\" alt=\"Logistic-regression-and-linear-regression\" border=\"0\" /></a>\n",
    "\n",
    "#### 5.2.6.1 Logistic regression equation\n",
    "\n",
    "By simple transformation, the logistic regression equation can be written in terms of an odds ratio.\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/tpSvfWf/logistic-formula.png\" alt=\"logistic-formula\" border=\"0\" /></a>\n",
    "\n",
    "\n",
    "#### 5.2.6.2 Clear advantage of logistic regression\n",
    "\n",
    "In essence, a logistic equation is created in such a way that the output values can only be between 0 and 1\n",
    "The logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead, or healthy/sick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1284c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.205008Z",
     "iopub.status.idle": "2021-10-20T14:34:55.205614Z",
     "shell.execute_reply": "2021-10-20T14:34:55.205379Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.205355Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# LogisticRegression\n",
    "    if logistic:\n",
    "        start = time.time()\n",
    "        model = LogisticRegression(max_iter=10000)\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        result[\"BoW_LogisticRegression\"] = score\n",
    "        print(\"Score for LogisticRegression classifier with fold={} > {} %\".format(k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_LogisticRegression\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\logistic_reg_calibrated.joblib\")\n",
    "elif model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\logistic_reg_calibrated.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d362a1a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.206856Z",
     "iopub.status.idle": "2021-10-20T14:34:55.207475Z",
     "shell.execute_reply": "2021-10-20T14:34:55.207245Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.207222Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if logistic or model_load:\n",
    "    predicted = calibrated.predict(X_test)\n",
    "    predicted_prob = calibrated.predict_proba(X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07258084",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.7 Support Vector Machine\n",
    "\n",
    "SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data.\n",
    "\n",
    "\n",
    "<a href=\"https://ibb.co/c6YSXZ9\"><img src=\"https://i.ibb.co/0CsRV6L/Dim-transofrmation.png\" alt=\"Dim-transofrmation\" border=\"0\" /></a>\n",
    "<a href=\"https://ibb.co/n1C67wh\"><img src=\"https://i.ibb.co/wQsBSdD/SVM-Img.png\" alt=\"SVM-Img\" border=\"0\" /></a>\n",
    "\n",
    "\n",
    "#### 5.2.7.1 Advantage of SVM\n",
    "SVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables. The risk of overfitting is less in SVM, while Logistic regression is vulnerable to overfitting.\n",
    "The algorithm creates a hyperplane or line(decision boundary) which separates data into classes. It uses the kernel trick to find the best line separator (decision boundary that has same distance from the boundary point of both classes)\n",
    "\n",
    "#### 5.2.7.2 Risks using SVM\n",
    "\n",
    "In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform as we surely do not have linear independent matrix and SVM does not perform very well when the data set has a lot of noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40c257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T14:34:55.211132Z",
     "iopub.status.busy": "2021-10-20T14:34:55.210907Z",
     "iopub.status.idle": "2021-10-20T14:34:55.232572Z",
     "shell.execute_reply": "2021-10-20T14:34:55.230248Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.211106Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# Linear support vector machine\n",
    "    if svm:\n",
    "        start = time.time()\n",
    "         \n",
    "        model = LinearSVC(dual=False, tol=tolerance, max_iter=max_iter_runtime)\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        score = calibrated.score(X_test, y_test)\n",
    "        bow_calibrated_model = calibrated\n",
    "        result[\"BoW_SVM\"] = score\n",
    "        print(\"Score for Linear support vector machine with fold={} = {} %\".format(k_fold, score))\n",
    "        # CalibratedClassifierCV - \n",
    "        # This class uses cross-validation to both estimate the parameters of a classifier and subsequently calibrate a classifier\n",
    "        # fits a copy of the base estimator to the training subset, and calibrates it using the testing subset      \n",
    "        end = time.time()\n",
    "        time_dict[\"BoW_SVM\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\linear_svc_calibrated.joblib\")\n",
    "else:\n",
    "    if model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\linear_svc_calibrated.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2b85d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.233966Z",
     "iopub.status.idle": "2021-10-20T14:34:55.235076Z",
     "shell.execute_reply": "2021-10-20T14:34:55.234697Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.234671Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "if svm or model_load:\n",
    "    predicted = calibrated.predict(X_test)\n",
    "    predicted_prob = calibrated.predict_proba(X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3183e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2.8 Summary for BoW with tf-idf without SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c3337",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.236553Z",
     "iopub.status.idle": "2021-10-20T14:34:55.237154Z",
     "shell.execute_reply": "2021-10-20T14:34:55.236950Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.236928Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t\\t Summary for bag of words\\n\")\n",
    "print(\"\\t\\t\\t\\t\\t    Score \\t\\t Time  \\t\\tCost Effective Score\\n\")\n",
    "max_i = [0, None]\n",
    "for key in result:\n",
    "    if max_i[0]< np.round(result[key]/time_dict[key], 8):\n",
    "        max_i[0] = np.round(result[key]/time_dict[key], 8)\n",
    "        max_i[1] = key\n",
    "    print(\"{:<30} ---->     {:>10}\\t|\".format(key, np.round(result[key],5)) + \"{:>10} min\\t    |\".format(np.round(time_dict[key]/60, 3)) + \"{:>15}\".format(np.round(result[key]/time_dict[key], 8))) \n",
    "print(\"\\n\\n\\t\\tBest accuracy\")\n",
    "print(\"{:<22}: {:>22}\".format(max(result), result[max(result)]))\n",
    "print(\"\\n\\t     Cost Effective Score\")\n",
    "print(\"{:<22}: {:>22}\\n\".format(max_i[1], max_i[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5820b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### 5.2.8.1 SVM and MultinomialNB are the winners.\n",
    "After testing our TF-IDF on several learning models we can see that both SVM and MultinomialNB models have over 90% accuracy on our datasets.\n",
    "Two key observations that differ the models are:\n",
    "1. SVM reached the highest accuracy\n",
    "2. MultinomialNB was the fastest, thus it's best for real time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84503b47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5.3 SVD + BoW\n",
    "Let's try to evaluate the SVD form on the training models. Earlier we created new matrecies with full to -> 3000 dim reduction using SVD. \n",
    "It would definity make sense to evaluate SVM but not MultinomialNB on them. \n",
    "\n",
    "The reason is Naive Bayes with SVD or other matrix factorization is not such a great idea is:\n",
    "Naive Bayes based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "Having said that, we shall begin evaluating SVD with SVM, Logistic Reg and RandomForest \n",
    "### 5.3.1 SVD & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384f19c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.238511Z",
     "iopub.status.idle": "2021-10-20T14:34:55.238851Z",
     "shell.execute_reply": "2021-10-20T14:34:55.238698Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.238676Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# Linear support vector machine\n",
    "    if svm and svd:\n",
    "        start = time.time()\n",
    "         \n",
    "        model = LinearSVC(dual=False, tol=tolerance, max_iter=max_iter_runtime)\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(svd_bow_X_train, y_train)\n",
    "        score = calibrated.score(svd_bow_X_test, y_test)\n",
    "        result[\"SVD_BoW_SVM\"] = score\n",
    "        print(\"Score for Linear support vector machine with fold={} = {} %\".format(k_fold, score))\n",
    "        # CalibratedClassifierCV - \n",
    "        # This class uses cross-validation to both estimate the parameters of a classifier and subsequently calibrate a classifier\n",
    "        # fits a copy of the base estimator to the training subset, and calibrates it using the testing subset      \n",
    "        end = time.time()\n",
    "        time_dict[\"SVD_BoW_SVM\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\svd_linear_svc_calibrated.joblib\")\n",
    "else:\n",
    "    if model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\svd_linear_svc_calibrated.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678672dd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.240240Z",
     "iopub.status.idle": "2021-10-20T14:34:55.240579Z",
     "shell.execute_reply": "2021-10-20T14:34:55.240432Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.240409Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (svm and svd) or model_load:\n",
    "    predicted = calibrated.predict(svd_bow_X_test)\n",
    "    predicted_prob = calibrated.predict_proba(svd_bow_X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac6b04",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.3.2 SVD & LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ed46a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.241512Z",
     "iopub.status.idle": "2021-10-20T14:34:55.241836Z",
     "shell.execute_reply": "2021-10-20T14:34:55.241690Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.241668Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# LogisticRegression\n",
    "    if logistic and svd:\n",
    "        start = time.time()\n",
    "        model = LogisticRegression(max_iter=10000)\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(svd_bow_X_train, y_train)\n",
    "        score = calibrated.score(svd_bow_X_test, y_test)\n",
    "        svd_bow_calibrated = calibrated\n",
    "        result[\"SVD_BoW_LogisticRegression\"] = score\n",
    "        print(\"Score for LogisticRegression classifier with fold={} > {} %\".format(k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"SVD_BoW_LogisticRegression\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\SVD_BoW_logistic_reg_calibrated.joblib\")\n",
    "elif model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\SVD_BoW_logistic_reg_calibrated.joblib') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7c911",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.244316Z",
     "iopub.status.idle": "2021-10-20T14:34:55.244762Z",
     "shell.execute_reply": "2021-10-20T14:34:55.244619Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.244595Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (logistic and svd) or model_load:\n",
    "    predicted = calibrated.predict(svd_bow_X_test)\n",
    "    predicted_prob = calibrated.predict_proba(svd_bow_X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e4a0e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.3.3 SVD & RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a9747",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.246122Z",
     "iopub.status.idle": "2021-10-20T14:34:55.246714Z",
     "shell.execute_reply": "2021-10-20T14:34:55.246535Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.246509Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if bow_run:\n",
    "# RandomForestClassifier\n",
    "    if random_forest and svd:\n",
    "        start = time.time()\n",
    "        model = RandomForestClassifier()\n",
    "        calibrated = CalibratedClassifierCV(base_estimator=model,cv=k_fold)\n",
    "        calibrated.fit(svd_bow_X_train, y_train)\n",
    "        score = calibrated.score(svd_bow_X_test, y_test)\n",
    "        result[\"SVD_BoW_RandomForest\"] = score\n",
    "        print(\"Score for RandomForestClassifier classifier with fold={} > {} %\".format(k_fold, score))\n",
    "        end = time.time()\n",
    "        time_dict[\"SVD_BoW_RandomForest\"] = end - start\n",
    "        if save:\n",
    "            dump(calibrated, \"saved_runs\\\\tfidf\\\\SVD_BoW_RandomForestClassifier_calibrated.joblib\")\n",
    "elif model_load:\n",
    "        calibrated = load(input_path+'saved_runs\\\\tfidf\\\\SVD_BoW_RandomForestClassifier_calibrated.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995672f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.248562Z",
     "iopub.status.idle": "2021-10-20T14:34:55.249050Z",
     "shell.execute_reply": "2021-10-20T14:34:55.248800Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.248775Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (andom_forest and svd) or model_load:\n",
    "    predicted = calibrated.predict(svd_bow_X_test)\n",
    "    predicted_prob = calibrated.predict_proba(svd_bow_X_test)\n",
    "\n",
    "    res1, res2 = map(list, zip(*predicted_prob))\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, res1, multi_class=\"ovr\")\n",
    "    print(\"Accuracy:{:^20}\".format(round(accuracy,2)))\n",
    "    print(\"Auc:{:^30}\".format(round(auc,2)))\n",
    "    print(\"Detail:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, predicted)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "                cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "           yticklabels=classes, title=\"Confusion matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.gcf().set_size_inches(11.5, 4)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "    ## Plot roc\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3, \n",
    "    label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(fpr, tpr))\n",
    "                   )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "              xlabel='False Positive Rate', \n",
    "              ylabel=\"True Positive Rate (Recall)\", \n",
    "              title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ## Plot precision-recall curve\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                     y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3, \n",
    "                   label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                      metrics.auc(recall, precision))\n",
    "                  )\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "              ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6826bce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.3.4 Summary for BoW with tf-idf with SVD\n",
    "\n",
    "We see that by implementing SVD on BoW matrix (without NGrams) we managed to reduce the columns dim by a factor of over *10 but still maintain good results.\n",
    "Also, the models files generated are more compact and hold compressed information. This is all very useful if we need to load them on a remote device. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede8dae",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.250232Z",
     "iopub.status.idle": "2021-10-20T14:34:55.251046Z",
     "shell.execute_reply": "2021-10-20T14:34:55.250836Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.250812Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\t\\t\\t Summary for bag of words\\n\")\n",
    "print(\"\\t\\t\\t\\t\\t    Score \\t\\t Time  \\t\\tCost Effective Score\\n\")\n",
    "max_i = [0, None]\n",
    "for key in result:\n",
    "    if max_i[0]< np.round(result[key]/time_dict[key], 8):\n",
    "        max_i[0] = np.round(result[key]/time_dict[key], 8)\n",
    "        max_i[1] = key\n",
    "    print(\"{:<30} ---->     {:>10}\\t|\".format(key, np.round(result[key],5)) + \"{:>10} min\\t    |\".format(np.round(time_dict[key]/60, 3)) + \"{:>15}\".format(np.round(result[key]/time_dict[key], 8))) \n",
    "print(\"\\n\\n\\t\\tBest accuracy\")\n",
    "print(\"{:<22}: {:>22}\".format(max(result), result[max(result)]))\n",
    "print(\"\\n\\t     Cost Effective Score\")\n",
    "print(\"{:<22}: {:>22}\\n\".format(max_i[1], max_i[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5ad47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5.4 Random testing \n",
    "Now we are ready try to input the model some random string sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560f4e4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.252141Z",
     "iopub.status.idle": "2021-10-20T14:34:55.252760Z",
     "shell.execute_reply": "2021-10-20T14:34:55.252601Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.252576Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_text_a = [\"You are shit\"]\n",
    "sample_text_b = [\"Fuck\"]\n",
    "sample_text_c = [\"I love you\"]\n",
    "sample_text_d = [\"Idan is the best teach in the Open University\"]\n",
    "sample_text_e = [\"I suck the water outside of the ship\"]\n",
    "sample_text_f = [\"I drain the water outside of the ship\"]\n",
    "sample_text_g = [\"The data I am using might or might not be refined to be better\"]\n",
    "\n",
    "\n",
    "list_of_samples = [sample_text_a, sample_text_b, sample_text_c, sample_text_d, sample_text_e, sample_text_f, sample_text_g]\n",
    "\n",
    "print(\"SVD Model - LogisticRegression\")\n",
    "def _get_prob(prob):\n",
    "  return prob[1]\n",
    "\n",
    "def bow_probability(texts):\n",
    "  return np.apply_along_axis(_get_prob, 1, bow_calibrated_model.predict_proba(bag_of_words.transform(texts)))  \n",
    "    \n",
    "def svd_probability(texts):\n",
    "  return np.apply_along_axis(_get_prob, 1, svd_bow_calibrated.predict_proba(svd.transform(bag_of_words.transform(texts))))  \n",
    "\n",
    "\n",
    "# Print the result per sample\n",
    "for sample in list_of_samples:\n",
    "    #print(np.abs(1-calibrated.predict(svd.transform(sample))), end=\"\")\n",
    "    prob = svd_probability(sample)\n",
    "    if prob[0] >= 0.5:\n",
    "        print(\"Negative {}%: {}\".format(int(np.round(prob[0], 2)*100), sample))\n",
    "    else:\n",
    "        print(\"Positive {}%: {}\".format(int(np.round(1-prob[0], 2)*100), sample))\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nNon SVD Model - SVM\")\n",
    "for sample in list_of_samples:\n",
    "    #print(np.abs(1-calibrated.predict(svd.transform(sample))), end=\"\")\n",
    "    prob = bow_probability(sample)\n",
    "    if prob[0] >= 0.5:\n",
    "        print(\"Negative {}%: {}\".format(int(np.round(prob[0], 2)*100), sample))\n",
    "    else:\n",
    "        print(\"Positive {}%: {}\".format(int(np.round(1-prob[0], 2)*100), sample))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fe7b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### It can be seen that \"I suck the water outside of the ship\" failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9294a32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 6. Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688db16",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.1 Method 2 - Word2Vec - Continuous bag-of-words (CBoW) \n",
    "\n",
    "Continuous bag-of-words (CBoW) is exactly the same as BoW, but instead of using sparse vectors to represent words, it uses dense vectors (continuous distributional \"embeddings\")\n",
    "\n",
    "\n",
    "\n",
    "#### 6.1.1 The CBoW and Skip-gram\n",
    "\n",
    "CBoW predicts the middle word based on surrounding context words. The context consists of a few words before and after our predicted word.\n",
    "\n",
    "Continuous Skip-gram Model predicts words within a certain range before and after the current word in the same sentence. In some sense, Skip-gram is just the opposite of CBoW. Given a word, we would want to predict the following and preceding likely sequences.\n",
    "\n",
    "<a href=\"https://ibb.co/gR7Rk7S\"><img src=\"https://i.ibb.co/pLPLYPf/skipgram-cbow.png\" alt=\"skipgram-cbow\" border=\"0\" /></a>\n",
    "\n",
    "Left  -- CBoW, ---- Prediction of \"W\" based on (w+n, ..., w-1, w+1, ..., w+n)\n",
    "\n",
    "Right - SkipGram, Prediction of (w+n, ..., w-1, w+1, ..., w+n) based on \"W\"\n",
    "\n",
    "\n",
    "#### 6.1.2 Why CBoW and not BoW\n",
    "\n",
    "1. BoW models encode every word in the vocabulary as one-hot-encoded vector i.e. for vocabulary of size  |V| , each word is represented by a  |V|  dimensional sparse vector with  1  at index corresponding to the word and  0  at every other index. As vocabulary may potentially run into millions, bag of word models face scalability challenges\n",
    "2. BoW models don’t respect semantics of the word. For example: words ‘car’ and ‘automobile’ are often used in the same context. However, the vectors corresponding to these words are orthogonal in bag of words model. The problem become more serious while modeling sentences. Ex: “Buy used cars” and “Purchase old automobiles” are represented by orthogonal vectors in BoW model\n",
    "3. While modeling phrases using BoW the order of words in the phrase is not respected. Ex: “This is good” and “Is this good” have exactly the same vector representation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 6.1.3 How does it work\n",
    "\n",
    "We take the sentence “Computer Science Program” where both “Computer” and “Program” are context words, and “Science” is the target word. We have a shallow network with a single hidden layer.\n",
    "1. D is the dimension size of the vector we can choose on, based on how dense we want it to be.\n",
    "2. V is a one-hot encoded vectorsize of (vocabulary / total number of unique words) with only a single 1 per v{i}\n",
    "3. We randomly initialize Embedding vector(E) with size V * D.\n",
    "4. We multiply the input one-hot encoded vector with the weights/embedding vector. This gives the embedding vectors for the context words (Natural and processing) of size 1 D\n",
    "\n",
    "<a href=\"https://ibb.co/H7Y4Pv8\"><img src=\"https://i.ibb.co/LS6Rgw7/cbow-vector.png\" alt=\"cbow-vector\" border=\"0\" /></a>\n",
    "\n",
    "\n",
    "In the hidden layer, we average the embedding vectors for the context words which forms the input for this layer of size 1 * D. \n",
    "This is multiplied by another Vector called Context Vector (E’) with size D * V. \n",
    "This gives us a vector of 1 * V which is then passed through a sigmoid function to get the final output\n",
    "\n",
    "\n",
    "The final output is compared with the one-hot encoded vector of Language (the middle word) [0, 1, 0, ..., 0] and the loss function is calculated. This loss is backpropagated and the model is trained using Gradient Descent.\n",
    "\n",
    "For Skip-gram, it’s just the opposite way around. \n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/74Gw729/skip-gram.png\" alt=\"skip-gram\" border=\"0\" /></a>\n",
    "\n",
    "\n",
    "In both cases, we just keep the Embedding(E) vector at the end.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 6.1.4 Complexity \n",
    "\n",
    "We will use pretrained word embeddings from the \n",
    "Unlike the BoW with the Sparse matrix, CBoW training takes much more time. \n",
    "But why should we not learn our own embeddings? Well, learning word embeddings from scratch is a challenging problem due to two primary reasons\n",
    "Gensim also has a directory of pre-trained embeddings which are \n",
    "\n",
    "\n",
    "\n",
    "#### 6.1.5 Impelmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d246d2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.254021Z",
     "iopub.status.idle": "2021-10-20T14:34:55.254533Z",
     "shell.execute_reply": "2021-10-20T14:34:55.254376Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.254353Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cbow_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85e2dc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.255557Z",
     "iopub.status.idle": "2021-10-20T14:34:55.256102Z",
     "shell.execute_reply": "2021-10-20T14:34:55.255933Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.255889Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import downloader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "w2v_model = downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54640cce",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.257084Z",
     "iopub.status.idle": "2021-10-20T14:34:55.257571Z",
     "shell.execute_reply": "2021-10-20T14:34:55.257425Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.257406Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embedding_feats(list_of_lists, DIMENSION, w2v_model):\n",
    "    zeros_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    missing = set()\n",
    "    missing_sentences = set()\n",
    "    #Traverse over each sentence\n",
    "    for tokens in tqdm(list_of_lists):\n",
    "        # Initially assign zeroes as the embedding vector for the sentence\n",
    "        feat_for_this = zeros_vector\n",
    "        #Count the number of words in the embedding for this sentence\n",
    "        count_for_this = 0\n",
    "        #Traverse over each word of a sentence\n",
    "        for token in tokens:\n",
    "            #Check if the word is in the embedding vector\n",
    "            if token in w2v_model:\n",
    "                #Add the vector of the word to vector for the sentence\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "            #Else assign the missing word to missing set just to have a look at it\n",
    "            else:\n",
    "                missing.add(token)\n",
    "        #If no words are found in the embedding for the sentence\n",
    "        if count_for_this == 0:\n",
    "            #Assign all zeroes vector for that sentence\n",
    "            feats.append(feat_for_this)\n",
    "            #Assign the missing sentence to missing_sentences just to have a look at it\n",
    "            missing_sentences.add(' '.join(tokens))\n",
    "        #Else take average of the values of the embedding for each word to get the embedding of the sentence\n",
    "        else:\n",
    "            feats.append(feat_for_this/count_for_this)\n",
    "    return feats, missing, missing_sentences\n",
    "\n",
    "train_vectors, missing, missing_sentences = embedding_feats(balanced[\"sentences\"], 300, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bffbd3e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.258459Z",
     "iopub.status.idle": "2021-10-20T14:34:55.258773Z",
     "shell.execute_reply": "2021-10-20T14:34:55.258632Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.258610Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cbow_X_train,cbow_X_test,y_train,y_test=train_test_split(train_vectors,y,test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336d8b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.2 Method 3 - ELMO\n",
    "\n",
    "Deep contextualized word representation\n",
    "    \n",
    "ELMO uses word vectors that are learned functions of the internal states of a deep bidirectional language model (biLM) pretrained on a large text corpus.\n",
    "The representations can be further added to existing models in any downstream tasks and also significantly improve SOTA across most challenging NLP problems\n",
    " \n",
    "The vectors are in particular modelled using both:     \n",
    "1. Syntaxical and semantical characteristics of word\n",
    "2. How use of these characteristics vary across linguistic contexts\n",
    "\n",
    "ELMo language model is designed using a deep bidirectional LSTM language model for learning words and their context. The deep BiLSTM architecture allows ELMo to learn more context-dependent aspects of word meanings in the higher layers along with syntax aspects in lower layers. This results in better word embeddings, and different representations of a word depending on the context it appears in (especially useful for homographs).\n",
    "\n",
    "### 6.2.1 Advantages of ELMo emebeddings:**\n",
    "1. ELMo vectors can be utilized in any NLP task directly as features or as a embedding layer and this typically results in vast gains in performance comapred to traditional NLP and ML models.\n",
    "2. ELMo vectors are trained using a large scale language model seeks to compute the probability of a word, given some prior history of words seen. This allow to utilize the context of the word in the sentence to build the entire sentence embedding.\n",
    " - Example: Apple in 'Apple is the best smartphone out there' vs. 'Eating apple a day keeps the doctor away' gets different representation as the full sentence is used to build the embeedings.\n",
    "3. ELMo language model is trained on a sizable dataset (1B Word Benchmark) with large-scale LSTM layers containing 4096 units & input embedding transform using 2048 convolutional filters which resulted in rich representations.\n",
    "4. Elmo is based on character level embeddings using character convolutions and can handle out of vocabulary words.\n",
    "\n",
    "### 6.2.2 Risks of using ELMo emebeddings in NLP tasks:**\n",
    "1. Need to Ffne tune the word vectors for downstream tasks\n",
    "2. Hyper-param tuning the down stream models for NLP task at hand \n",
    "3. Computationally intensive and not easier to exaplain the predictions\n",
    "\n",
    "\n",
    "<a href=\"https://ibb.co/G0tqp0y\"><img src=\"https://i.ibb.co/80dHs0q/ELMO-architecture.png\" alt=\"ELMO-architecture\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d219dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.2.3 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e173b3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.260365Z",
     "iopub.status.idle": "2021-10-20T14:34:55.260854Z",
     "shell.execute_reply": "2021-10-20T14:34:55.260665Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.260624Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, gc\n",
    "from tqdm import tqdm\n",
    "import re, time, pickle\n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "import tensorflow.compat.v1 as tf1\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from cachetools import TTLCache,cached\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "display(HTML(data=\"\"\"<style> div#notebook-container{width:95%;}</style>\"\"\"))\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else: \n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf1.set_random_seed(seed)\n",
    "    \n",
    "seed_everything(2047)\n",
    "tf1.disable_eager_execution()\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "def encode(le, labels):\n",
    "    # Encoding output labels in keras long format for loss compatability\n",
    "    enc = le.transform(labels)\n",
    "    return tf.keras.utils.to_categorical(enc)\n",
    "\n",
    "def decode(le, one_hot):\n",
    "    # Decoding the keras encoded labels back \n",
    "    dec = np.argmax(one_hot, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "def compute_metrics_v1(y_t,y_p):  \n",
    "    # Helper function for computing metrics on the dataset\n",
    "    accuracy = accuracy_score(y_true=y_t,y_pred=y_p)\n",
    "    recall = recall_score(y_true=y_t,y_pred=y_p)\n",
    "    precision = precision_score(y_true=y_t,y_pred=y_p)\n",
    "    f1 = f1_score(y_true=y_t,y_pred=y_p)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \n",
    "\n",
    "def plot_model_history(model,model_name,skip_start:int=5,skip_end:int=1,metrics=['loss'],freq=1):\n",
    "    # This function can be used to plot train vs validation losses as model training is done\n",
    "    # If the nbr_epochs are less (<10), callbacks can be implemented and plotted suitably for a sample epoch run\n",
    "    skip_end = skip_end if skip_end > 0 else 1\n",
    "    fig, ax = plt.subplots(1,len(metrics),figsize=(15,4.25))\n",
    "    fig.suptitle('Training history for '+model_name, fontsize=14)\n",
    "    for i,metric in enumerate(metrics):\n",
    "        \n",
    "        losses = model.history.history[metric]\n",
    "        val_losses = model.history.history['val_'+metric]\n",
    "        lrs = range(len(losses))[skip_start:-skip_end]\n",
    "\n",
    "        axis = ax if len(metrics) == 1 else ax[i]\n",
    "        axis.plot(lrs,losses[skip_start:-skip_end])\n",
    "        axis.plot(lrs,val_losses[skip_start:-skip_end])\n",
    "        ax.set_xticks(ax.get_xticks()[::freq])\n",
    "        axis.set_ylabel(metric)\n",
    "        axis.set_xlabel('epoch')\n",
    "        axis.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# Ingest and process data\n",
    "df = balanced.rename(columns={'sentences':'sentence','Negative':'label'})\n",
    "\n",
    "# Encode the labels for binary loss function \n",
    "x = list(df['sentence'])\n",
    "y = list(df['label'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "\n",
    "x_enc = x\n",
    "y_enc = encode(le, y)\n",
    "\n",
    "# Train test (20%) split\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.asarray(x_enc), np.asarray(y_enc), test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47829319",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.2.4 Visualize sample ELMo word vectors and projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73317049",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.261744Z",
     "iopub.status.idle": "2021-10-20T14:34:55.262454Z",
     "shell.execute_reply": "2021-10-20T14:34:55.262267Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.262240Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "def elmo_vectors(x):\n",
    "    embeddings = embed(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(tf.compat.v1.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))\n",
    "\n",
    "pos_idx_vis = df[df.label==1].sample(n=100).index.values\n",
    "neg_idx_vis = df[df.label==0].sample(n=100).index.values\n",
    "\n",
    "# list_train = [train[i:i+100] for i in range(0,train.shape[0],100)]\n",
    "# list_test = [test[i:i+100] for i in range(0,test.shape[0],100)]\n",
    "\n",
    "# elmo_train = [elmo_vectors(df['setence']) for x in list_train]\n",
    "# elmo_test = [elmo_vectors(df['sentence']) for x in list_test]\n",
    "\n",
    "# elmo_train_new = np.concatenate(elmo_train, axis = 0)\n",
    "# elmo_test_new = np.concatenate(elmo_test, axis = 0)\n",
    "\n",
    "pos_sentences_vec = elmo_vectors(df[df.index.isin(pos_idx_vis)]['sentence'])\n",
    "neg_sentences_vec = elmo_vectors(df[df.index.isin(neg_idx_vis)]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1a518",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.263585Z",
     "iopub.status.idle": "2021-10-20T14:34:55.264251Z",
     "shell.execute_reply": "2021-10-20T14:34:55.264067Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.264040Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(25,10))       \n",
    "sns.heatmap(pos_sentences_vec[:10,:50], vmin=0, vmax=1,linewidths=.5,cmap=\"YlGnBu\",ax=ax[0]);\n",
    "sns.heatmap(neg_sentences_vec[:10,:50], vmin=0, vmax=1,linewidths=.5,cmap=\"YlGnBu\",ax=ax[1]);\n",
    "ax[0].set_title('Positive lables');\n",
    "ax[1].set_title('Negative labels');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e1974",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.265390Z",
     "iopub.status.idle": "2021-10-20T14:34:55.265700Z",
     "shell.execute_reply": "2021-10-20T14:34:55.265562Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.265540Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tsne(X,n_comp=2,n_iter=1000,verbose=0,seed=2047):\n",
    "    tsne = TSNE(n_components=n_comp, n_iter=n_iter, verbose=verbose, random_state=seed)\n",
    "    tsne_embed = tsne.fit_transform(X.astype(np.float32))\n",
    "    projection = pd.DataFrame(columns=['embed_1', 'embed_2'], data=tsne_embed) \n",
    "    return projection\n",
    "\n",
    "elmo_vector_projection = get_tsne(np.concatenate([pos_sentences_vec,neg_sentences_vec]),n_iter=1000)\n",
    "elmo_vector_projection['label'] = list(df[df.index.isin(pos_idx_vis)]['label'])+list(df[df.index.isin(neg_idx_vis)]['label'])\n",
    "\n",
    "# sns.scatterplot(data=elmo_vector_projection, x=\"embed_1\", y=\"embed_2\", hue=\"label\")\n",
    "fig = px.scatter(elmo_vector_projection,x='embed_1',y='embed_2',color='label',\n",
    "                 title=\"TSNE projections of elmo embedded vectors\")\n",
    "# fig.update_layout(width=1500,height=1000)\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570cefb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/2qXNwv0/neural-layers.png\" alt=\"neural-layers\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1177746",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b662e71",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea37eb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.267071Z",
     "iopub.status.idle": "2021-10-20T14:34:55.267617Z",
     "shell.execute_reply": "2021-10-20T14:34:55.267451Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.267430Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get ElMo embeddings as the input layer and build the MLP on top of it for classification\n",
    "\n",
    "embed = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "def ELMoEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n",
    "\n",
    "def get_elmo_mlp_model(params={}):\n",
    "    \n",
    "    szs = params.get('szs',[64,32])\n",
    "    drop_trainable = params.get('drop_trainable',False)\n",
    "    drops = params.get('drops',[0.1,0.1])\n",
    "    bilstm = params.get('bilstm',None)\n",
    "    n_label = params.get('n_label',2)\n",
    "    act_out = params.get('act_out','softmax')\n",
    "    reg_dense = params.get('reg_dense',None)\n",
    "    h_reg = tf.keras.regularizers.L2(l2=0.01) if reg_dense is True else None\n",
    "        \n",
    "    # Build input, outputs, build the model and compile losees\n",
    "    x = None\n",
    "    input_text = tf.keras.layers.Input(shape=(1,), dtype='string',name='input_text')\n",
    "    embedding = tf.keras.layers.Lambda(ELMoEmbedding,output_shape = (1024,), name = 'elmo_embedding')(input_text)\n",
    "    \n",
    "    if x is None: x = embedding\n",
    "    if bilstm is True:\n",
    "        x = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(1024,\n",
    "                 return_sequences=False,\n",
    "                 recurrent_dropout=0,\n",
    "                 dropout=0,\n",
    "                 name=\"BiLSTM\"))(x)\n",
    "    \n",
    "    for i,sz in enumerate(szs):\n",
    "        x = tf.keras.layers.Dense(\n",
    "            sz,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=h_reg)(x)\n",
    "        if drop_trainable is True: \n",
    "            x = tf.keras.layers.Dropout(drops[i])(x)\n",
    "\n",
    "#     x = tf.keras.layers.Dense(64,activation='relu')(embedding)\n",
    "#     x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "    predictions = tf.keras.layers.Dense(n_label,activation=act_out)(x)\n",
    "    model = tf.keras.Model(inputs = [input_text],outputs = predictions)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_elmo_mlp_model()\n",
    "model.summary()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(tf.compat.v1.tables_initializer())\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test,y_test),\n",
    "        epochs=1,\n",
    "        batch_size=32)\n",
    "    model.save_weights(r\"saved_runs\\elmo\\\"+'elmo-mlp-model_soft.h5\")\n",
    "    \n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(tf.compat.v1.tables_initializer())\n",
    "    model.load_weights(r\"saved_runs\\elmo\\\"+'elmo-mlp-model_soft.h5\")\n",
    "    predicts = model.predict(x_test, batch_size=32)\n",
    "    \n",
    "\n",
    "                       \n",
    "                       \n",
    "y_test_d = decode(le, y_test)\n",
    "y_preds_d = decode(le, predicts)\n",
    "\n",
    "print(predicts.shape)\n",
    "print(y_test_d.shape)\n",
    "print(y_preds_d.shape)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test_d, y_preds_d))\n",
    "print(metrics.classification_report(y_test_d, y_preds_d))\n",
    "print(\"Accuracy of ELMO MLP is:\",accuracy_score(y_test_d,y_preds_d))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_d, y_preds_d)\n",
    "    ax[0].plot(\n",
    "        fpr, tpr, lw=3,label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(fpr, tpr)) )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], xlabel='FPR',ylabel=\"TPR (Recall)\", title=\"ROC Curve\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test_d, y_preds_d)\n",
    "    ax[1].plot(\n",
    "        recall, precision, lw=3,\n",
    "        label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(recall, precision)) )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.gcf().set_size_inches(12, 4)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "print(end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221d8fa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.268691Z",
     "iopub.status.idle": "2021-10-20T14:34:55.269034Z",
     "shell.execute_reply": "2021-10-20T14:34:55.268865Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.268846Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"You are shit\",\n",
    "    \"Fuck\",\n",
    "    \"I love you\",\n",
    "    \"Idan is the best teach in the Open University\",\n",
    "    \"I suck the water outside of the ship\",\n",
    "    \"I drain the water outside of the ship\",\n",
    "    \"The data I am using might or might not be refined to be better\"]\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(tf.compat.v1.tables_initializer())\n",
    "    model.load_weights('/kaggle/working/'+'elmo-mlp-model_soft.h5')\n",
    "    predicts = model.predict(np.array(test_texts), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e6cc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.3 Method 4 - BERT\n",
    "\n",
    "BERT is Bidirectional Encoder Representations from Transformers)\n",
    "- Bert model builds on top of bidirectional idea from ELMo, but uses the relatively new transformer architecture to compute word embeddings. It has been shown to produce excellent word embeddings, achieving state-of-the-art results on various NLP tasks.  \n",
    "- BERT also builds on top of a number of clever ideas bubbling up in NLP community recently like, Semi-supervised Sequence Learning, ELMo, ULMFiT, OpenAI transformer & Transformers (Vaswani et al).\n",
    "\n",
    "### 6.3.1 Transformers\n",
    "\n",
    "- Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. The idea behind Transformer is to handle the dependencies between input and output with attention and recurrence completely.\n",
    "- It is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. Transduction is the conversion of input sequences into output sequences. \n",
    "- Transformer's SOTA achieved results on tasks such as machine translation started to make some in the field think of them as a replacement to LSTMs, as they also deal with long-term dependancies better than LSTMs.\n",
    "- Encoder-Decoder structure of transformer made it perfect for machine translation and downstream tasks such as sentence classification with some fine-tuning utilizing a pre-trained models.\n",
    "\n",
    "<a href=\"https://ibb.co/vvvj88Q\"><img src=\"https://i.ibb.co/JFFsPPy/transformer-diag.png\" alt=\"transformer-diag\" border=\"0\" /></a>\n",
    "\n",
    "### 6.3.2 Tokenization \n",
    "- Tokenization: Breaking a word down into one or more sub-words referenced in the model vocabulary. Transformers models in general, and BERT and DistilBERT in particular, use tokenization.\n",
    "- Example: the sentence “My name is Marisha” is tokenized into [CLS] My name is Maris ##ha [SEP], which is represented by the vector [101, 1422, 1271, 1110, 27859, 2328, 102].\n",
    "- Hugging Face provides a series of pre-trained tokenizers for different models. I've used the pretrained DistilBERT tokenizer for the task.\n",
    "- Same tokenizer is used to tokenize the training and testing datasets and converted them to huggingface dataset format and then back to PyTorch tensor format used during training.\n",
    "\n",
    "### 6.3.3 BERT for sentence classification \n",
    "- We have built a senetence classifier model utilizing distillbert that has near identical peroformance on almost all the NLP tasks with less computation power usage. The dataset is processed, tokenized and pre-trained model is loaded and a custom layer is built on top of pre-trained model for further fine tuning and evaluation.\n",
    "- Utility of fine tuning: The word vectors from pre-trained BERT models give contexual representations that are tailored for generic context and NLP task at hand. With fine tuning, the vocabulary in the custom dataset can be updated based on output layer activations. This helps in better handling of the supervised classifications.\n",
    "\n",
    "<a href=\"https://ibb.co/y5L6mVM\"><img src=\"https://i.ibb.co/hHpBxCn/Bert-architecture-diagram.png\" alt=\"Bert-architecture-diagram\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529da8e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.270388Z",
     "iopub.status.idle": "2021-10-20T14:34:55.270707Z",
     "shell.execute_reply": "2021-10-20T14:34:55.270570Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.270549Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import transformers\n",
    "import torch\n",
    "import random, gc\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "display(HTML(data=\"\"\"<style> div#notebook-container{width:95%;}</style>\"\"\"))\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(transformers.__version__)\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(2047)\n",
    "transformers.set_seed(2047)\n",
    "\n",
    "\n",
    "def compute_metrics_v1(p): \n",
    "    # Custom metric function to feed into sentence classfier model\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \n",
    "\n",
    "def compute_metrics_v2(y_t,y_p):    \n",
    "    # Generic version of metric function for evaluatiuon\n",
    "    accuracy = accuracy_score(y_true=y_t,y_pred=y_p)\n",
    "    recall = recall_score(y_true=y_t,y_pred=y_p)\n",
    "    precision = precision_score(y_true=y_t,y_pred=y_p)\n",
    "    f1 = f1_score(y_true=y_t,y_pred=y_p)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \n",
    "\n",
    "def encode_text(df): \n",
    "    # Pre-trained huggingface auto tokenizer \n",
    "    return tokenizer(df['sentence'], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "\n",
    "df = balanced.rename(columns={'sentences':'sentence','Negative':'label'})\n",
    "\n",
    "# Bert Tokenizer and pre-trained model\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# Convert the pandas datasets to hf datasets and encode based on map \n",
    "dataset = Dataset.from_pandas(df[['sentence','label']])\n",
    "# dataset = load_dataset('csv', data_files='file_path.csv')\n",
    "encoded_dataset = dataset.map(encode_text,batched=True,load_from_cache_file=False)\n",
    "\n",
    "# Train, valid, test split \n",
    "test_sz = 0.2\n",
    "valid_sz = 0.1\n",
    "\n",
    "test_idx = df.sample(frac=test_sz,random_state=2047).index\n",
    "valid_idx = df.loc[~df.index.isin(test_idx)].sample(frac=valid_sz,random_state=2047).index\n",
    "train_idx = df.loc[(~df.index.isin(test_idx))&(~df.index.isin(valid_idx))].index\n",
    "\n",
    "train_frame = Dataset.from_pandas(pd.DataFrame.from_dict(encoded_dataset[list(train_idx.values)]))\n",
    "valid_frame = Dataset.from_pandas(pd.DataFrame.from_dict(encoded_dataset[list(valid_idx.values)]))\n",
    "test_frame = Dataset.from_pandas(pd.DataFrame.from_dict(encoded_dataset[list(test_idx.values)]))\n",
    "\n",
    "# COnvert to torch format \n",
    "columns_to_return = ['input_ids', 'label', 'attention_mask']\n",
    "train_frame.set_format(type='torch', columns=columns_to_return)\n",
    "valid_frame.set_format(type='torch', columns=columns_to_return)\n",
    "test_frame.set_format(type='torch', columns=columns_to_return)\n",
    "\n",
    "# ! pip install bertviz\n",
    "from bertviz import head_view, model_view\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "def show_head_view(model, tokenizer, text):\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention = model(input_ids)[-1]\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "    head_view(attention, tokens)\n",
    "    \n",
    "def show_model_view(model, tokenizer, text):\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention = model(input_ids)[-1]\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "    model_view(attention, tokens)\n",
    "    \n",
    "\n",
    "model_version = 'distilbert-base-uncased'\n",
    "do_lower_case = True\n",
    "model = DistilBertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "\n",
    "# Model params \n",
    "train_batch_sz = 32\n",
    "eval_batch_sz = 32\n",
    "metric_name = \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "n_sz_max_word = df['Number of words'].max()\n",
    "num_labels = 2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=train_batch_sz,\n",
    "    per_device_eval_batch_size=eval_batch_sz,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,             \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "#     push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_frame,\n",
    "    eval_dataset=valid_frame,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_v1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2aaea3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.271686Z",
     "iopub.status.idle": "2021-10-20T14:34:55.272039Z",
     "shell.execute_reply": "2021-10-20T14:34:55.271866Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.271846Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evalaute predictions\n",
    "test_preds = trainer.predict(test_frame)\n",
    "\n",
    "test_preds_p = pd.DataFrame(test_preds.predictions)\n",
    "test_preds_p['pred'] = np.argmax(test_preds_p.values, axis=1)\n",
    "test_preds_p['label'] = list(test_preds.label_ids)\n",
    "\n",
    "test_met = compute_metrics_v2(test_preds_p['label'].values,test_preds_p['pred'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51198194",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.273151Z",
     "iopub.status.idle": "2021-10-20T14:34:55.273455Z",
     "shell.execute_reply": "2021-10-20T14:34:55.273314Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.273293Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds_p.groupby(['pred','label']).size()\n",
    "print(test_met['accuracy'])\n",
    "test_preds.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5058f38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4370d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.274796Z",
     "iopub.status.idle": "2021-10-20T14:34:55.275137Z",
     "shell.execute_reply": "2021-10-20T14:34:55.274998Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.274976Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_test_d = decode(le, y_test)\n",
    "y_preds_d = decode(le, predicts)\n",
    "\n",
    "print(predicts.shape)\n",
    "print(y_test_d.shape)\n",
    "print(y_preds_d.shape)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test_d, y_preds_d))\n",
    "print(metrics.classification_report(y_test_d, y_preds_d))\n",
    "print(\"Accuracy of ELMO MLP is:\",accuracy_score(y_test_d,y_preds_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7ff3b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.276477Z",
     "iopub.status.idle": "2021-10-20T14:34:55.276778Z",
     "shell.execute_reply": "2021-10-20T14:34:55.276640Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.276618Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_d, y_preds_d)\n",
    "    ax[0].plot(\n",
    "        fpr, tpr, lw=3,label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(fpr, tpr)) )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], xlabel='FPR',ylabel=\"TPR (Recall)\", title=\"ROC Curve\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test_d, y_preds_d)\n",
    "    ax[1].plot(\n",
    "        recall, precision, lw=3,\n",
    "        label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(recall, precision)) )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.gcf().set_size_inches(12, 4)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "print(end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b256b4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 6.2 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc54946",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.277638Z",
     "iopub.status.idle": "2021-10-20T14:34:55.277973Z",
     "shell.execute_reply": "2021-10-20T14:34:55.277792Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.277772Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evalaute predictions\n",
    "test_preds = trainer.predict(test_frame)\n",
    "\n",
    "test_preds_p = pd.DataFrame(test_preds.predictions)\n",
    "test_preds_p['pred'] = np.argmax(test_preds_p.values, axis=1)\n",
    "test_preds_p['label'] = list(test_preds.label_ids)\n",
    "\n",
    "test_met = compute_metrics_v2(test_preds_p['label'].values,test_preds_p['pred'].values)\n",
    "\n",
    "test_preds_p.groupby(['pred','label']).size()\n",
    "print(test_met['accuracy'])\n",
    "test_preds.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e4cc6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Image to text - recognize samples so we can later use our model to evaluate them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea26f4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-20T14:34:55.279048Z",
     "iopub.status.idle": "2021-10-20T14:34:55.279344Z",
     "shell.execute_reply": "2021-10-20T14:34:55.279207Z",
     "shell.execute_reply.started": "2021-10-20T14:34:55.279186Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample from \"http://digitalnativestudios.com/textmeshpro/docs/rich-text/\"\n",
    "source_path = r\"materials\\images\\in\\image_text_sample.png\"\n",
    "\n",
    "image = cv2.imread(source_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Remove some noises, this should be later changed to be dynamic \n",
    "gray = cv2.medianBlur(gray, 3)\n",
    "\n",
    "source_path_out = \"\\\\\".join(source_path.split(\"\\\\\")[0:-2])\n",
    "filename = source_path_out + \"\\\\out\\\\\" + \"image_out.png\"\n",
    "cv2.imwrite(filename, gray)\n",
    "\n",
    "# The line below runs pytesseract, changing the stdout to text file that can be later loaded to the code above \n",
    "# text = pytesseract.image_to_string(Image.open(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391993b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "[Google OCR](https://github.com/tesseract-ocr/tesseract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cab26a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Voice to text - recognize samples so we can later use our model to evaluate them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15dbc1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "###### Under development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18edc8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "###### Found a cool open-source library to help split the audio into freq bins (fft) and a dataset that helps analyze feelings according the the sound (frequences, rate of change and etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbae45f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "[librosa - OpenSource](https://librosa.org/doc/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53cd50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "[Data set](\"https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bd06f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# To do list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf67397",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "1. Add Bert and ELMO\n",
    "2. Finialize the features\n",
    "3. Clean the data better (preprocessing for the data)\n",
    "4. Make a conclusion and \"in a nutshell segments\" \n",
    "5. Finilze notebook look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b9d2d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.843703,
   "end_time": "2021-10-20T14:52:21.190580",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-20T14:51:45.346877",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
